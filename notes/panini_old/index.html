<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="&ldquo;The simplest of several competing explanations is likely to be the correct one&rdquo; - Occam&rsquo;s Razor
 Intelligence as compression Let’s examine a modified version of Searle’s Chinese room experiment."><meta property="og:title" content="The Pāṇinian Approach to World Modelling"><meta property="og:description" content="&ldquo;The simplest of several competing explanations is likely to be the correct one&rdquo; - Occam&rsquo;s Razor
 Intelligence as compression Let’s examine a modified version of Searle’s Chinese room experiment."><meta property="og:type" content="website"><meta property="og:image" content="https://sibeshkar.github.io/icon.png"><meta property="og:url" content="https://sibeshkar.github.io/notes/panini_old/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Pāṇinian Approach to World Modelling"><meta name=twitter:description content="&ldquo;The simplest of several competing explanations is likely to be the correct one&rdquo; - Occam&rsquo;s Razor
 Intelligence as compression Let’s examine a modified version of Searle’s Chinese room experiment."><meta name=twitter:image content="https://sibeshkar.github.io/icon.png"><meta name=twitter:site content="sibeshkar"><title>The Pāṇinian Approach to World Modelling</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://sibeshkar.github.io//icon.png><link href=https://sibeshkar.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://sibeshkar.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://sibeshkar.github.io/js/darkmode.57cc7b5e8e492d275b93efd99423df30.min.js></script>
<script src=https://sibeshkar.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://sibeshkar.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://sibeshkar.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://sibeshkar.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://sibeshkar.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://sibeshkar.github.io/",fetchData=Promise.all([fetch("https://sibeshkar.github.io/indices/linkIndex.8c5cafe27a13e3618180c8686fb29358.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://sibeshkar.github.io/indices/contentIndex.efceedee9587dff1114458d146812b73.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://sibeshkar.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://sibeshkar.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/sibeshkar.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=sibeshkar.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://sibeshkar.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://sibeshkar.github.io/>👾 skar</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>The Pāṇinian Approach to World Modelling</h1><p class=meta>Last updated
Feb 13, 2025
<a href=https://github.com/sibeshkar/notes/panini_old.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#intelligence-as-compression>Intelligence as compression</a></li><li><a href=#pāṇinis-razor>Pāṇini&rsquo;s Razor</a></li></ol></nav></details></aside><blockquote><p>&ldquo;The simplest of several competing explanations is likely to be the correct one&rdquo; - Occam&rsquo;s Razor</p></blockquote><a href=#intelligence-as-compression><h2 id=intelligence-as-compression><span class=hanchor arialabel=Anchor># </span>Intelligence as compression</h2></a><p>Let’s examine a modified version of Searle’s Chinese room experiment. Suppose we have 5.6TB of text data, ethically scraped from the internet, broken into word pairs (2-grams) stored in a lookup table. When asked to complete &ldquo;I was going to wear a&mldr;&rdquo;, it might meaninglessly output &ldquo;a lot&rdquo; because &ldquo;a lot&rdquo; appears more frequently than &ldquo;a shirt&rdquo; or &ldquo;a skirt&rdquo;. A 3-gram model, using two words of context, improves accuracy but still fails in cases like &ldquo;It’s raining outside, wear a&mldr;&rdquo;. Both are examples of generative models: they predict the statistically-most-likely next word based on patterns in the data.</p><p>Large Language Models (LLMs) like DeepSeek, GPT, or Claude are more sophisticated versions of such generative models. They use thousands of tokens of context and the &lsquo;attention&rsquo; mechanism to focus on relevant parts of the input to &lsquo;query&rsquo; with. They&rsquo;re not just storing a memorized table of what comes after what, but using compute to extract and store a hiearchy of reusable chunks of information in it&rsquo;s layers. A result is that the weights of the Llama-65B model occupy around 365GB on disk, down from the 5.6TB it&rsquo;s trained on (a 14x compression)[1]. We can see that generalization ability and data efficiency are equivalent: generalization comes from squeezing every bit of information out of your datapoints, understanding all correlations and causations, and connecting all the dots. “Squeezing every bit of information” is meant literally: generalization is the very direct result of compression.</p><p>Yet, LLMs are still many orders-of-magnitude less data-efficient than humans. Lee Sedol, a top Go player, played around 10,000 games in his lifetime, while DeepMind&rsquo;s bot AlphaGo required 30 million games to match him. If Sedol had played 30 million games, how skilled would he be? What would a human who has absorbed all of human knowledge look like? How does the human generative model compress information so effectively? The answer to these question is the key to building machines that think, learn, adapt to tasks like (or better than) humans do, i.e. general machine intelligence.</p><p>But it&rsquo;s hard to run this experiment because most humans have seen orders of magnitude less data in their lifetimes, and it&rsquo;s hard to manually inspect human priors. Or so I thought, until I attended the lecture series[2] by Dr. Saroja Bhate at Bangalore International Centre, on Pāṇini (pronounced &ldquo;pah-nee-nee&rdquo;), the ancient Sanskrit grammarian. Over 2300 years ago, before the advent of computers or formal logic, Pāṇini sat down and methodically reduced all of human knowledge, then floating around in spoken Vedic Sanskrit, into a generative grammar of exactly 3,995 <em>sūtras</em>, or rewrite rules - recorded in his magnum opus, the <em>Aṣṭādhyāyī</em>. These rules have remained unchanged ever since.</p><p>There is a small pool of postdoctoral Sanskrit scholars who are fortunate to understand the full magnitude of Pāṇini&rsquo;s achievement, but I will try to do some justice to it. Faced with a large corpus of spoken Vedic and contemporary Sanskrit, many thousands of hours of audio signals collected without any substrate to record with or automated tooling to work with, Pāṇini, over 12 years, found abstracted atoms of meaning that when combined with a set of dynamic rules and meta-rules formed a generative grammar, a deterministic state machine that could be used to re-synthesize the original audio corpus - and be recited in just 2 hours (an astonishing compression ratio of atleast ~5000:1)[3].</p><p>How is Pāṇini&rsquo;s ~4000 rule set a compressed generative model like one would understand an LLM to be? Imagine prompting an LLM like ChatGPT with an empty prompt, and letting it run for a few paragraphs. If you did this a trillion times, with slightly different temperature settings, you would eventually recover a slightly morphed version of the entire corpus it was trained on. &lsquo;Model distillation&rsquo; is the industry term for this common practice, used to copy parts of another LLM&rsquo;s training set. In a similar manner, if you were to run the state machine described in the <em>Aṣṭādhyāyī</em> a billion times with random inputs, you would eventually recover all the Vedic hymns, mantras, and Brahmanas it compresses, include the different variations of spoken Sanskrit. Of course, just like you guide an LLM&rsquo;s output by including an input prompt, you would need to guide the state machine in the <em>Aṣṭādhyāyī</em> to generate the <em>kind</em> of sentences you wanted. But the complexity of the original source remains contained in the abstracted, highly compressed rule set.</p><blockquote><p>&ldquo;If the universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset&rdquo; - Ray Solomonoff, inventor of algorithmic probability</p></blockquote><p>Pāṇini gives the formation of every inflected, compounded, or derived word, with an exact statement of the sound-variations (including accent) and of the meaning. His foresight in designing these rules means they have stood the test of time - not only expressive enough to explain the knowledge of the past at the time, but also to generalize to phrases and sentences in the future since then. His work is the first formal system known to man, doing to linguistic reality what Euclid would go on to do later for geometry, but it would be no overstatement to call it the most impressive human act of knowledge compression till date. It is the only example we have of true optimal
<a href=https://en.m.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference rel=noopener>Solomonoff induction</a>, finding the shortest executable archive of a dataset, as evidenced by it&rsquo;s durability over time.</p><p>Of course, Pāṇini was working with Sanskrit, whose underlying structure makes it less context-sensitive than English and more amenable to such decomposition. But for the sake of comparison, his methods if automated and applied to the
<a href=http://prize.hutter1.net/ rel=noopener>Hutter compression prize</a> dataset could compress 1GB of Wikipedia data to a few kilobytes (down from the current record of 110MB as of Feb 2025). A digital superintelligence in action, would very much employ what could be called &ldquo;Pāṇini&rsquo;s Razor&rdquo; - applying the dual techniques of abstraction and economy ruthlessly to thousands of noisy signals of various forms and fidelity, reducing them by many orders of magnitude into succinct set of formal predicates - deriving an explanation unchanging in time. Like breaking down a house into basic Lego-like blocks and then building a new house from it back gain, a machine like this could then combine these discovered reusable concepts on the fly using abstracted transformation rules to generalize to any unknown.</p><blockquote><p>&ldquo;The descriptive grammar of Sanskrit which Pāṇini brought to it&rsquo;s highest perfection is one of the greatest monuments of human intelligence&rdquo; - L. Bloomfield, father of American distributionalism</p></blockquote><p>It&rsquo;s worth examining the implications of this. To build powerful thinking machines of the future that can compress information and generalize better than humans do, we must dig up and study in great detail the forgotten relic that is Pāṇini&rsquo;s Razor.</p><a href=#pāṇinis-razor><h2 id=pāṇinis-razor><span class=hanchor arialabel=Anchor># </span>Pāṇini&rsquo;s Razor</h2></a><p>Disclaimer : I have no formal training in Sanskrit, and the following is merely a programmer&rsquo;s attempt to reverse-engineer Pāṇini&rsquo;s methods of compression. All mistakes are mine, all brilliance his.</p><p>This is largely a guide for programmers, so we dive into code right away. Let&rsquo;s first look at how generative grammars compress information.</p><p>[to be continued&mldr;] Source[4]</p><p>[1]
<a href="https://www.youtube.com/watch?v=dO4TPJkeaaU" rel=noopener>Youtube video</a>, Compression for AGI, Jack Rae, Stanford MLSys, ex-OpenAI, 2023</p><p>[2]
<a href="https://www.youtube.com/playlist?list=PLsAPTmdVuspykLNnjs1_zQKRMqRRfDr2R" rel=noopener>Youtube Playlist</a>, Pāṇini Lecture Series, Dr. Saroja Bhate, Bangalore International Center, 2023</p><p>[3] The <em>Aṣṭādhyāyī</em> achieves a remarkable compression ratio of at least 5000:1, condensing the rules that can generate over 10,000 hours of attested Sanskrit literature (including the ~20,000 verses of the four Vedas, along with the 100,000 verses of the Mahabharata, 24,000 verses of the Ramayana, 400,000 verses of the Puranas, and hundreds of thousands of verses across texts, philosophical shastras, and classical poetry) into just 2 hours of precisely formulated rules.</p><p>[4] Pāṇini: Catching the Ocean in a Cow’s Hoofprint, Vikram Chandra, 2019
<a href=https://blog.granthika.co/panini/ rel=noopener>blog.granthika.co/panini/</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://sibeshkar.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Sibesh Kar using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2025</p><ul><li><a href=https://sibeshkar.github.io/>Home</a></li><li><a href=https://twitter.com/sibeshkar>Twitter</a></li></ul></footer></div></div></body></html>
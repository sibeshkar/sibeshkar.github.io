{"/":{"title":"üëæ skar","content":"founder/ceo @ [mayalabs.io](https://mayalabs.io) - applied research lab on a mission to build machines which will programs themselves to do any task.\n\nareas of interest:\n\n1. symbolic AI \u0026 program synthesis\n2. cognitive science\n3. ontologies in physics\n4. self-replicating machines\n5. non-dualism\n5. game engines\n6. graphic novels\n\nalways up to discuss these [questions](notes/questions). making time to grok other fringe theories : sibesh[at]mayalabs[dot]io. \n\n\n## machine intelligence\n- [public list of predictions](https://x.com/sibeshkar/status/1709316722359198017)\n- [the parable of the robot pirate](/notes/pirate)\n- [unbounded generality](/notes/unbounded)\n- [on function approximation](https://twitter.com/sibeshkar/status/1615804999463997441)\n- [benchmarking generalization in machines](https://blog.mayalabs.io/benchmark/)\n- [general machine intelligence](https://blog.mayalabs.io/general-machine-intelligence/)\n- [biological intelligence \u0026 generalization](https://x.com/sibeshkar/status/1871525755387326782)\n- [on an alien planet](/notes/alien)\n- [charkhas, ai \u0026 economic growth](/notes/economy)\n- [the PƒÅ·πáinian approach to compression](/notes/panini)\n\n## modelling physics\n- [the non-local intuition of david bohm](notes/bohm)\n- [the physics of causality](notes/causality)\n- [this magic trick has no secrets](notes/no-secrets)\n\n## purpose\n- [questions](notes/questions)\n- [non-dualism](notes/nd)\n\n\n\n\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/alien":{"title":"On An Alien Planet","content":"\nA good way to determine what the first thinking machine will look like is to think about what it will need to do to adapt and survive on an alien planet.\n\nAn 'alien planet' is any new environment - the deep sea Mariana Trench was alien to him, yet James Cameron, an evolved primate with land-based mammalian training, could engineer his way down to it.\n\nAt the extremes, one may think of a yet undiscovered planet that exists in no internet corpus of today. Something right out of Netflix's Alien Worlds.\n\nThis is a data question, an energy question, and a hardware assembly question, rolled into one.\n\nHow does it learn from very little data?\nHow does it operate with very little energy? \nHow does it reorganize/repair from very few building blocks?\n\nMiss answers to any one of these questions, and the system dies by default. The human brain is able to deal all of them, hence one instantiation of it - James Cameron - can make it to the Mariana Trench.\n\nToday's machine learning systems answer none of these questions. Such is the state of modern AI.\n\nLest it may seem daunting, it is no extraordinary ask. Mere 6 year olds adapt to such alien worlds every day, every week. Playing and mastering new and alien video games with commonplace ease that would stump any machine.\n\nIndeed each new frontier is like a deep sea trench.\n\nAnd for each new explorer, man, but moreso machine, each unanswered question is a fatal festering wound.","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/CJK-+-Latex-Support-%E6%B5%8B%E8%AF%95":{"title":"CJK + Latex Support (ÊµãËØï)","content":"\n## Chinese, Japanese, Korean Support\nÂá†‰πéÂú®Êàë‰ª¨ÊÑèËØÜÂà∞‰πãÂâçÔºåÊàë‰ª¨Â∑≤ÁªèÁ¶ªÂºÄ‰∫ÜÂú∞Èù¢„ÄÇ\n\nÏö∞Î¶¨Í∞Ä Í∑∏Í≤ÉÏùÑ ÏïåÍ∏∞ÎèÑ Ï†ÑÏóê Ïö∞Î¶¨Îäî ÎïÖÏùÑ Îñ†ÎÇ¨ÏäµÎãàÎã§.\n\nÁßÅ„Åü„Å°„Åå„Åù„Çå„ÇíÁü•„Çã„Åª„ÅºÂâç„Å´„ÄÅÁßÅ„Åü„Å°„ÅØÂú∞Èù¢„ÇíÈõ¢„Çå„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ\n\n## Latex\n\nBlock math works with two dollar signs `$$...$$`\n\n$$f(x) = \\int_{-\\infty}^\\infty\n    f\\hat(\\xi),e^{2 \\pi i \\xi x}\n    \\,d\\xi$$\n\t\nInline math also works with single dollar signs `$...$`. For example, Euler's identity but inline: $e^{i\\pi} = -1$\n\nAligned equations work quite well:\n\n$$\n\\begin{aligned}\na \u0026= b + c \\\\ \u0026= e + f \\\\\n\\end{aligned}\n$$\n\nAnd matrices\n\n$$\n\\begin{bmatrix}\n1 \u0026 2 \u0026 3 \\\\\na \u0026 b \u0026 c\n\\end{bmatrix}\n$$\n\n## RTL\nMore information on configuring RTL languages like Arabic in the [config](config.md) page.\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/Editing+and+formatting/Callouts).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/config":{"title":"Configuration","content":"\n## Configuration\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Jacky Zhao\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: false\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true \n\n# whether to render copy buttons for code blocks\nenableCodeBlockCopy: true \n\n# whether to render callouts\nenableCallouts: true\n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flashes of unstyled content and improves\n# smoothness of Quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: false\n\n# whether to display an 'edit' button next to the last edited field\n# that links to github\nenableGitHubEdit: true\nGitHubLink: https://github.com/jackyzha0/quartz/tree/hugo/content\n\n# whether to render mermaid diagrams\nenableMermaid: true\n\n# whether to use Operand to power semantic search\n# IMPORTANT: replace this API key with your own if you plan on using\n# Operand search!\nsearch:\n  enableSemanticSearch: false\n  operandApiKey: \"REPLACE-WITH-YOUR-OPERAND-API-KEY\"\n  operandIndexId: \"REPLACE-WITH-YOUR-OPERAND-INDEX-ID\"\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title:\n  \"ü™¥ Quartz 3.3\"\n\n# links to show in the footer\nlinks:\n  - link_name: Twitter\n    link: https://twitter.com/_jzhao\n  - link_name: Github\n    link: https://github.com/jackyzha0\n```\n\n### Code Block Titles\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n    ```yaml {title=\"data/config.yaml\", linenos=false}\n    enableCodeBlockTitle: true\n    ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n      ```markdown {linenos=false}\n       ```yaml {title=\"data/config.yaml\"}\n       enableCodeBlockTitle: true  # example from step 1\n       ```\n      ```\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur, and the title attribute will be ignored.\n\n### HTML Favicons\nIf you would like to customize the favicons of your Quartz-based website, you \ncan add them to the `data/config.yaml` file. The **default** without any set \n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your \n`data/config.yaml` file. For example, here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the \n**Apple touch icon** to Quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your \nwebpage to the home screen of their Apple device. If you are interested in more \ninformation about the current and past standards of favicons, you can read \n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href` \nattribute, are relative to the `static/` directory.\n\n### Graph View\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n    # whether automatically generate a legend\n    enableLegend: false\n    \n    # whether to allow dragging nodes in the graph\n    enableDrag: true\n    \n    # whether to allow zooming and panning the graph\n    enableZoom: true\n    \n    # how many neighbours of the current node to show (-1 is all nodes)\n    depth: 1\n    \n    # initial zoom factor of the graph\n    scale: 1.2\n    \n    # how strongly nodes should repel each other\n    repelForce: 2\n\n    # how strongly should nodes be attracted to the center of gravity\n    centerForce: 1\n\n    # what the default link length should be\n    linkDistance: 1\n    \n    # how big the node labels should be\n    fontSize: 0.6\n    \n    # scale at which to start fading the labes on nodes\n    opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n\t# same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n\n## Styling\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`. \n\n### Partials\nPartials are what dictate what gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](troubleshooting.md).\n\n## Language Support\n[CJK + Latex Support (ÊµãËØï)](CJK%20+%20Latex%20Support%20(ÊµãËØï).md) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'ŸÖÿØŸàŸÜÿ™Ÿä'\n    weight = 1\n```\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/custom-Domain":{"title":"Custom Domain","content":"\n### Registrar\nThis step is only applicable if you are using a **custom domain**! If you are using a `\u003cYOUR-USERNAME\u003e.github.io` domain, you can skip this step.\n\nFor this last bit to take effect, you also need to create a CNAME record with the DNS provider you register your domain with (i.e. NameCheap, Google Domains).\n\nGitHub has some [documentation on this](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site), but the tldr; is to\n\n1. Go to your forked repository (`github.com/\u003cYOUR-GITHUB-USERNAME\u003e/quartz`) settings page and go to the Pages tab. Under \"Custom domain\", type your custom domain, then click **Save**.\n2. Go to your DNS Provider and create a CNAME record that points from your domain to `\u003cYOUR-GITHUB-USERNAME.github.io.` (yes, with the trailing period).\n\n\t![Example Configuration for Quartz](/notes/images/google-domains.png)*Example Configuration for Quartz*\n3. Wait 30 minutes to an hour for the network changes to kick in.\n4. Done!","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/docker":{"title":"Hosting with Docker","content":"\nIf you want to host Quartz on a machine without using a webpage hosting service, it may be easier to [install Docker Compose](https://docs.docker.com/compose/install/) and follow the instructions below than to [install Quartz's dependencies manually](preview%20changes.md).\n## Hosting Quartz Locally\nYou can serve Quartz locally at `http://localhost:1313` with the following script, replacing `/path/to/quartz` with the \nactual path to your Quartz folder.\n\ndocker-compose.yml\n```\nservices:\n  quartz-hugo:\n    image: ghcr.io/jackyzha0/quartz:hugo\n    container_name: quartz-hugo\n    volumes:\n      - /path/to/quartz:/quartz\n    ports:\n      - 1313:1313\n\n    # optional\n    environment:\n      - HUGO_BIND=0.0.0.0\n      - HUGO_BASEURL=http://localhost\n      - HUGO_PORT=1313\n      - HUGO_APPENDPORT=true\n      - HUGO_LIVERELOADPORT=-1\n```\n\nThen run with: `docker-compose up -d` in the same directory as your `docker-compose.yml` file.\n\nWhile the container is running, you can update the `quartz` fork with: `docker exec -it quartz-hugo make update`.\n\n## Exposing Your Container to the Internet\n\n### To Your Public IP Address with Port Forwarding (insecure)\n\nAssuming you are already familiar with [port forwarding](https://en.wikipedia.org/wiki/Port_forwarding) and [setting it up with your router model](https://portforward.com):\n\n1. You should set the environment variable `HUGO_BASEURL=http://your-public-ip` and then start your container.\n2. Set up port forwarding on your router from port `p` to `your-local-ip:1313`.\n3. You should now be able to access Quartz from outside your local network at `http://your-public-ip:p`.\n\nHowever, your HTTP connection will be unencrypted and **this method is not secure**.\n\n### To a Domain using Cloudflare Proxy\n\n1. Port forward 443 (HTTPS) from your machine.\n2. Buy a custom domain (say, `your-domain.com`) from [Cloudflare](https://www.cloudflare.com/products/registrar/). Point a DNS A record from `your-domain.com` to your public IP address and enable the proxy.\n3. Set the environment variables `HUGO_BASEURL=https://your-domain.com`, `HUGO_PORT=443`, and `HUGO_APPENDPORT=false`. Change `1313:1313` to `443:443` for the `ports` in `docker-compose.yml`.\n4. Spin up your Quartz container and enjoy it at `https://your-domain.com`!\n\n### To a Domain using a Reverse Proxy\n\nIf you want to serve more than just Quartz to the internet on this machine (or don't want to use the Cloudflare registrar and proxy), you should follow the steps in the section above (as appropriate) and also set up a reverse proxy, like [Traefik](https://doc.traefik.io/traefik). Be sure to configure your TLS certificates too!\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/editing":{"title":"Editing Content in Quartz","content":"\n## Editing \nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\n### Folder Structure\nHere's a rough overview of what's what.\n\n**All content in your garden can found in the `/content` folder.** To make edits, you can open any of the files and make changes directly and save it. You can organize content into any folder you'd like.\n\n**To edit the main home page, open `/content/_index.md`.**\n\n### Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so, otherwise the generated page will not have a title!\n\nYou can also add tags here as well.\n\n```yaml\n---\ntitle: \"Example Title\"\ntags:\n- example-tag\n---\n\nRest of your content here...\n```\n\n### Obsidian\nI recommend using [Obsidian](http://obsidian.md/) as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of your local files.\n\nThis step is **highly recommended**.\n\n\u003e üîó Step 3: [How to setup your Obsidian Vault to work with Quartz](obsidian.md)\n\n## Previewing Changes\nThis step is purely optional and mostly for those who want to see the published version of their digital garden locally before opening it up to the internet. This is *highly recommended* but not required.\n\n\u003e üëÄ Step 4: [Preview Quartz Changes](preview%20changes.md)\n\nFor those who like to live life more on the edge, viewing the garden through Obsidian gets you pretty close to the real thing.\n\n## Publishing Changes\nNow that you know the basics of managing your digital garden using Quartz, you can publish it to the internet!\n\n\u003e üåç Step 5: [Hosting Quartz online!](hosting.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](troubleshooting.md).\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/hosting":{"title":"Deploying Quartz to the Web","content":"\n## Hosting on GitHub Pages\nQuartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! Follow the steps below.\n\n### Enable GitHub Actions Permissions\nBy default, GitHub disables workflows from modifying your files (for good reason!). However, Quartz needs this to write the actual site files back to GitHub.\n\nHead to `Settings \u003e Action \u003e General \u003e Workflow Permissions` and choose `Read and Write Permissions`\n\n![[notes/images/github-actions.png]]\n*Enable GitHub Actions*\n\n### Enable GitHub Pages\n\nHead to the 'Settings' tab of your forked repository and go to the 'Pages' tab.\n\n1. (IMPORTANT) Set the source to deploy from `master` (and not `hugo`) using `/ (root)`\n2. Set a custom domain here if you have one!\n\n![Enable GitHub Pages](/notes/images/github-pages.png)*Enable GitHub Pages*\n\n### Pushing Changes\nTo see your changes on the internet, we need to push it them to GitHub. Quartz is a `git` repository so updating it is the same workflow as you would follow as if it were just a regular software project.\n\n```shell\n# Navigate to Quartz folder\ncd \u003cpath-to-quartz\u003e\n\n# Commit all changes\ngit add .\ngit commit -m \"message describing changes\"\n\n# Push to GitHub to update site\ngit push origin hugo\n```\n\nNote: we specifically push to the `hugo` branch here. Our GitHub action automatically runs everytime a push to is detected to that branch and then updates the `master` branch for redeployment.\n\n### Setting up the Site\nNow let's get this site up and running. Never hosted a site before? No problem. Have a fancy custom domain you already own or want to subdomain your Quartz? That's easy too.\n\nHere, we take advantage of GitHub's free page hosting to deploy our site. Change `baseURL` in `/config.toml`. \n\nMake sure that your `baseURL` has a trailing `/`!\n\n[Reference `config.toml` here](https://github.com/jackyzha0/quartz/blob/hugo/config.toml)\n\n```toml\nbaseURL = \"https://\u003cYOUR-DOMAIN\u003e/\"\n```\n\nIf you are using this under a subdomain (e.g. `\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz`), include the trailing `/`. **You need to do this especially if you are using GitHub!**\n\n```toml\nbaseURL = \"https://\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz/\"\n```\n\nChange `cname` in `/.github/workflows/deploy.yaml`. Again, if you don't have a custom domain to use, you can use `\u003cYOUR-USERNAME\u003e.github.io`.\n\nPlease note that the `cname` field should *not* have any path `e.g. end with /quartz` or have a trailing `/`.\n\n[Reference `deploy.yaml` here](https://github.com/jackyzha0/quartz/blob/hugo/.github/workflows/deploy.yaml)\n\n```yaml {title=\".github/workflows/deploy.yaml\"}\n- name: Deploy  \n  uses: peaceiris/actions-gh-pages@v3  \n  with:  \n\tgithub_token: ${{ secrets.GITHUB_TOKEN }} # this can stay as is, GitHub fills this in for us!\n\tpublish_dir: ./public  \n\tpublish_branch: master\n\tcname: \u003cYOUR-DOMAIN\u003e\n```\n\nHave a custom domain? [Learn how to set it up with Quartz ](custom%20Domain.md).\n\n### Ignoring Files\nOnly want to publish a subset of all of your notes? Don't worry, Quartz makes this a simple two-step process.\n\n‚ùå [Excluding pages from being published](ignore%20notes.md)\n\n## Docker Support\nIf you don't want to use a hosting service, you can host using [Docker](docker.md) instead!\nI would *not use this method* unless you know what you are doing.\n\n---\n\nNow that your Quartz is live, let's figure out how to make Quartz really *yours*!\n\n\u003e Step 6: üé® [Customizing Quartz](config.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](troubleshooting.md).\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/ignore-notes":{"title":"Ignoring Notes","content":"\n### Quartz Ignore\nEdit `ignoreFiles` in `config.toml` to include paths you'd like to exclude from being rendered.\n\n```toml\n...\nignoreFiles = [  \n    \"/content/templates/*\",  \n    \"/content/private/*\", \n    \"\u003cyour path here\u003e\"\n]\n```\n\n`ignoreFiles` supports the use of Regular Expressions (RegEx) so you can ignore patterns as well (e.g. ignoring all `.png`s by doing `\\\\.png$`).\nTo ignore a specific file, you can also add the tag `draft: true` to the frontmatter of a note.\n\n```markdown\n---\ntitle: Some Private Note\ndraft: true\n---\n...\n```\n\nMore details in [Hugo's documentation](https://gohugo.io/getting-started/configuration/#ignore-content-and-data-files-when-rendering).\n\n### Global Ignore\nHowever, just adding to the `ignoreFiles` will only prevent the page from being access through Quartz. If you want to prevent the file from being pushed to GitHub (for example if you have a public repository), you need to also add the path to the `.gitignore` file at the root of the repository.","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/obsidian":{"title":"Obsidian Vault Integration","content":"\n## Setup\nObsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.\n\n### New Vault\nIf you don't have an existing Vault, [download Obsidian](https://obsidian.md/) and create a new Vault in the `/content` folder that you created and cloned during the [setup](setup.md) step.\n\n### Linking an existing Vault\nThe easiest way to use an existing Vault is to copy all of your files (directory and hierarchies intact) into the `/content` folder.\n\n## Settings\nGreat, now that you have your Obsidian linked to your Quartz, let's fix some settings so that they play well.\n\nOpen Settings \u003e Files \u0026 Links and look for these two items:\n\n1. Set the **New link format** to **Absolute Path in vault**. If you have a completely flat vault (no folders), this step isn't necessary.\n2. Turn **on** the **Automatically update internal links** setting.\n\n\n![[notes/images/obsidian-settings.png]]*Obsidian Settings*\n\n## Templates\nInserting front matter everytime you want to create a new Note gets annoying really quickly. Luckily, Obsidian supports templates which makes inserting new content really easily.\n\n\u003e [!WARNING]\n\u003e \n\u003e **If you decide to overwrite the `/content` folder completely, don't remove the `/content/templates` folder!**\n\nHead over to Options \u003e Core Plugins and enable the Templates plugin. Then go to Options \u003e Hotkeys and set a hotkey for 'Insert Template' (I recommend `[cmd]+T`). That way, when you create a new note, you can just press the hotkey for a new template and be ready to go!\n\n\u003e üëÄ Step 4: [Preview Quartz Changes](preview%20changes.md)\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/philosophy":{"title":"Quartz Philosophy","content":"\n\u003e ‚Äú[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.‚Äù ‚Äî Richard Hamming\n\n## Why Quartz?\nHosting a public digital garden isn't easy. There are an overwhelming number of tutorials, resources, and guides for tools like [Notion](https://www.notion.so/), [Roam](https://roamresearch.com/), and [Obsidian](https://obsidian.md/), yet none of them have super easy to use *free* tools to publish that garden to the world.\n\nI've personally found that\n1. It's nice to access notes from anywhere\n2. Having a public digital garden invites open conversations\n3. It makes keeping personal notes and knowledge *playful and fun*\n\nI was really inspired by [Bianca](https://garden.bianca.digital/) and [Joel](https://joelhooks.com/digital-garden)'s digital gardens and wanted to try making my own.\n\n**The goal of Quartz is to make hosting your own public digital garden free and simple.** You don't even need your own website. Quartz does all of that for you and gives your own little corner of the internet.\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/preview-changes":{"title":"Preview Changes","content":"\nIf you'd like to preview what your Quartz site looks like before deploying it to the internet, the following\ninstructions guide you through installing the proper dependencies to run it locally.\n\n\n## Install `hugo-obsidian`\nThis step will generate the list of backlinks for Hugo to parse. Ensure you have [Go](https://golang.org/doc/install) (\u003e= 1.16) installed.\n\n```bash\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```\n\nIf you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly (see [[troubleshooting#`command not found: hugo-obsidian`|the troubleshooting page]])! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\n##  Installing Hugo\nHugo is the static site generator that powers Quartz. [Install Hugo with \"extended\" Sass/SCSS version](https://gohugo.io/getting-started/installing/) first. Then,\n\n```bash\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\n\n\u003e [!INFO] Docker Support\n\u003e\n\u003e If you have the Docker CLI installed already, you can avoid installing `hugo-obsidian` and `hugo`. Instead, open your terminal, navigate to your folder with Quartz and run `make docker`\n\nAfterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated! Now, let's get it hosted online.\n\n\u003e üåç Step 5: [Hosting Quartz online!](hosting.md)\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/search":{"title":"Search","content":"\nQuartz supports two modes of searching through content.\n\n## Full-text\nFull-text search is the default in Quartz. It produces results that *exactly* match the search query. This is easier to setup but usually produces lower quality matches.\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nenableSemanticSearch: false\n```\n\n## Natural Language\nNatural language search is powered by [Operand](https://beta.operand.ai/). It understands language like a person does and finds results that best match user intent. In this sense, it is closer to how Google Search works.\n\nNatural language search tends to produce higher quality results than full-text search.\n\nHere's how to set it up.\n\n1. Login or Register for a new Operand account. Click the verification link sent to your email, and you'll be redirected to the dashboard. (Note) You do not need to enter a credit card to create an account, or get started with the Operand API. The first $10 of usage each month is free. To learn more, see pricing. If you go over your free quota, we'll (politely) reach out and ask you to configure billing.\n2. Create your first index. On the dashboard, under \"Indexes\", enter the name and description of your index, and click \"Create Index\". Note down the ID of the index (obtained by clicking on the index name in the list of indexes), as you'll need it in the next step. IDs are unique to each index, and look something like `uqv1duxxbdxu`.\n3. Click into the index you've created. Under \"Index Something\", select \"SITEMAP\" from the dropdown and click \"Add Source\".\n4. For the \"Sitemap.xml URL\", put your deployed site's base URL followed by `sitemap.xml`. For example, for `quartz.jzhao.xyz`, put `https://quartz.jzhao.xyz/sitemap.xml`. Leave the URL Regex empty. \n5. Get your API key. On the dashboard, under \"API Keys\", you can manage your API keys. If you don't already have an API key, click \"Create API Key\". You'll need this for the next step.\n6. Open `data/config.yaml`. Set `enableSemanticSearch` to `true`, `operandApiKey` to your copied key, and `operandIndexId` to the ID of the index we created from earlier..\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nsearch:\n  enableSemanticSearch: true\n  operandApiKey: \"jp9k5hudse2a828z98kxd6z3payi8u90rnjf\"\n  operandIndexId: \"s0kf3bd6tldw\"\n```\n7. Push your changes to the site and wait for it to deploy.\n8. Check the Operand dashboard and wait for your site to index. Enjoy natural language search powered by Operand!\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/setup":{"title":"Setup","content":"\n## Making your own Quartz\nSetting up Quartz requires a basic understanding of `git`. If you are unfamiliar, [this resource](https://resources.nwplus.io/2-beginner/how-to-git-github.html) is a great place to start!\n\n### Forking\n\u003e A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\n\nNavigate to the GitHub repository for the Quartz project:\n\nüìÅ [Quartz Repository](https://github.com/jackyzha0/quartz)\n\nThen, Fork the repository into your own GitHub account. **Make sure that when you fork, you _uncheck_ the 'Copy the `hugo` branch only' option**.\n\nIf you don't have an account, you can make on for free [here](https://github.com/join). More details about forking a repo can be found on [GitHub's documentation](https://docs.github.com/en/get-started/quickstart/fork-a-repo).\n\n![[notes/images/fork.png]]\n\n### Cloning\nAfter you've made a fork of the repository, you need to download the files locally onto your machine. Ensure you have `git`, then type the following command in your terminal replacing `YOUR-USERNAME` with your GitHub username.\n\n```shell\ngit clone https://github.com/YOUR-USERNAME/quartz\n```\n\n## Editing\nGreat! Now you have everything you need to start editing and growing your digital garden. If you're ready to start writing content already, check out the recommended flow for editing notes in Quartz.\n\n\u003e ‚úèÔ∏è Step 2: [Editing Notes in Quartz](editing.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](troubleshooting.md).\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":["setup"]},"/notes/archive/showcase":{"title":"Showcase","content":"\nWant to see what Quartz can do? Here are some cool community gardens :)\n\n- [Quartz Documentation (this site!)](https://quartz.jzhao.xyz/)\n- [Jacky Zhao's Garden](https://jzhao.xyz/)\n- [Scaling Synthesis - A hypertext research notebook](https://scalingsynthesis.com/)\n- [AWAGMI Intern Notes](https://notes.awagmi.xyz/)\n- [Shihyu's PKM](https://shihyuho.github.io/pkm/)\n- [SlRvb's Site](https://slrvb.github.io/Site/)\n- [Course notes for Information Technology Advanced Theory](https://a2itnotes.github.io/quartz/)\n- [Brandon Boswell's Garden](https://brandonkboswell.com)\n- [Siyang's Courtyard](https://siyangsun.github.io/courtyard/)\n- [Data Dictionary üß†](https://glossary.airbyte.com/)\n- [sspaeti.com's Second Brain](https://brain.sspaeti.com/)\n- [oldwinter„ÅÆÊï∞Â≠óËä±Âõ≠](https://garden.oldwinter.top/)\n- [SethMB Work](https://sethmb.xyz/)\n- [Abhijeet's Math Wiki](https://abhmul.github.io/quartz/Math-Wiki/)\n- [Mike's AI Garden ü§ñü™¥](https://mwalton.me/)\n\nIf you want to see your own on here, submit a [Pull Request adding yourself to this file](https://github.com/jackyzha0/quartz/blob/hugo/content/notes/showcase.md)!\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/troubleshooting":{"title":"Troubleshooting and FAQ","content":"\nStill having trouble? Here are a list of common questions and problems people encounter when installing Quartz.\n\nWhile you're here, join our [Discord](https://discord.gg/cRFFHYye7t) :)\n\n### Does Quartz have Latex support?\nYes! See [CJK + Latex Support (ÊµãËØï)](CJK%20+%20Latex%20Support%20(ÊµãËØï).md) for a brief demo.\n\n### Can I use \\\u003cObsidian Plugin\\\u003e in Quartz?\nUnless it produces direct Markdown output in the file, no. There currently is no way to bundle plugin code with Quartz.\n\nThe easiest way would be to add your own HTML partial that supports the functionality you are looking for.\n\n### My GitHub pages is just showing the README and not Quartz\nMake sure you set the source to deploy from `master` (and not `hugo`) using `/ (root)`! See more in the [hosting](hosting.md) guide\n\n### Some of my pages have 'January 1, 0001' as the last modified date\nThis is a problem caused by `git` treating files as case-insensitive by default and some of your posts probably have capitalized file names. You can turn this off in your Quartz by running this command.\n\n```shell\n# in the root of your Quartz (same folder as config.toml)\ngit config core.ignorecase true\n\n# or globally (not recommended)\ngit config --global core.ignorecase true\n```\n\n### Can I publish only a subset of my pages?\nYes! Quartz makes selective publishing really easy. Heres a guide on [excluding pages from being published](ignore%20notes.md).\n\n### Can I host this myself and not on GitHub Pages?\nYes! All built files can be found under `/public` in the `master` branch. More details under [hosting](hosting.md).\n\n### `command not found: hugo-obsidian`\nMake sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize `hugo-obsidian` as an executable.\n\n```shell\n# Add the following 2 lines to your ~/.bash_profile (~/.zshrc if you are on Mac)\nexport GOPATH=/Users/$USER/go\nexport PATH=$GOPATH/bin:$PATH\n\n# In your current terminal, to reload the session\nsource ~/.bash_profile # again, (~/.zshrc if you are on Mac)\n```\n\n### How come my notes aren't being rendered?\nYou probably forgot to include front matter in your Markdown files. You can either setup [Obsidian](obsidian.md) to do this for you or you need to manually define it. More details in [the 'how to edit' guide](editing.md).\n\n### My custom domain isn't working!\nWalk through the steps in [the hosting guide](hosting.md) again. Make sure you wait 30 min to 1 hour for changes to take effect.\n\n### How do I setup analytics?\nQuartz by default uses [Plausible](https://plausible.io/) for analytics. \n\nIf you would prefer to use Google Analytics, you can follow this [guide in the Hugo documentation](https://gohugo.io/templates/internal/#google-analytics). \n\nAlternatively, you can also import your Google Analytics data into Plausible by [following this guide](https://plausible.io/docs/google-analytics-import).\n\n\n### How do I change the content on the home page?\nTo edit the main home page, open `/content/_index.md`.\n\n### How do I change the colours?\nYou can change the theme by editing `assets/custom.scss`. More details on customization and themeing can be found in the [customization guide](config.md).\n\n### How do I add images?\nYou can put images anywhere in the `/content` folder.\n\n```markdown\nExample image (source is in content/notes/images/example.png)\n![Example Image](/content/notes/images/example.png)\n```\n\n### My Interactive Graph and Backlinks aren't up to date\nBy default, the `linkIndex.json` (which Quartz needs to generate the Interactive Graph and Backlinks) are not regenerated locally. To set that up, see the guide on [local editing](editing.md)\n\n### Can I use React/Vue/some other framework?\nNot out of the box. You could probably make it work by editing `/layouts/_default/single.html` but that's not what Quartz is designed to work with. 99% of things you are trying to do with those frameworks you can accomplish perfectly fine using just vanilla HTML/CSS/JS.\n\n## Still Stuck?\nQuartz isn't perfect! If you're still having troubles, file an issue in the GitHub repo with as much information as you can reasonably provide. Alternatively, you can message me on [Twitter](https://twitter.com/_jzhao) and I'll try to get back to you as soon as I can.\n\nüêõ [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/archive/updating":{"title":"Updating","content":"\nHaven't updated Quartz in a while and want all the cool new optimizations? On Unix/Mac systems you can run the following command for a one-line update! This command will show you a log summary of all commits since you last updated, press `q` to acknowledge this. Then, it will show you each change in turn and press `y` to accept the patch or `n` to reject it. Usually you should press `y` for most of these unless it conflicts with existing changes you've made! \n\n```shell\nmake update\n```\n\nOr, if you don't want the interactive parts and just want to force update your local garden (this assumed that you are okay with some of your personalizations been overriden!)\n\n```shell\nmake update-force\n```\n\nOr, manually checkout the changes yourself.\n\n\u003e [!warning] Warning!\n\u003e\n\u003e If you customized the files in `data/`, or anything inside `layouts/`, your customization may be overwritten!\n\u003e Make sure you have a copy of these changes if you don't want to lose them.\n\n\n```shell\n# add Quartz as a remote host\ngit remote add upstream git@github.com:jackyzha0/quartz.git\n\n# index and fetch changes\ngit fetch upstream\ngit checkout -p upstream/hugo -- layouts .github Makefile assets/js assets/styles/base.scss assets/styles/darkmode.scss config.toml data \n```\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/bohm":{"title":"The Dangerous Intuition Of David Bohm","content":"\nA few months before Einstein died, he wrote about an unpopular intuition in physics which went against everything he considered sacred. An intuition surfaced by an otherwise ostracised physicist, David Bohm.\n\n\u003e ‚Äú[If it is true] then nothing will remain of my whole castle in the air including the theory of gravitation, but also nothing of the rest of contemporary physics.‚Äù\n\nTo Einstein, Bohm‚Äôs intuition was downright¬†_dangerous_.\n\nThe Twitter watering-hole erupted last week when a prominent union minister dismissed mathematical figures of a lagging economy by quoting how ‚Äúmath never helped Einstein discover gravity‚Äù. Does math help explain the economy? Perhaps. Did math really help Einstein (re)discover his equations of gravity? David Bohm, who this essay is about, would vehemently claim that it didn‚Äôt. That would be among the very few ideas on which both Einstein and Bohm would agree.\n\nTo understand this, one must first understand how a gyroscope works.\n\nOne way to understand it is to write down the three equations that the book tells you about. Make sure angular momentum is conserved on all sides. Then mechanically derive the final result. Formulaic. Works every time. Can be taught en masse. Very¬†_efficient._\n\nDavid Bohm was the kind of physicist who would never understand it that way. He would imagine himself as the gyroscope, standing on his toes, perched on a gimbal, arms stretched far, pirouetting swiftly in circles, making small muscular movements to keep his balance. He would¬†_feel_¬†the behaviour of the gyroscope, understanding it‚Äôs motion through some form of muscular interiorization. The math and formulae would come later, almost as a pedestrian way of communicating that insight to others. Second-hand intuition.\n\nBohm was a throwback to the earlier days of physics which was unusual even in his time ‚Äì An era of physics devoid of reams of astronomical data, quantum supercomputers, and well-intentioned ‚Äúpractical‚Äù problem solving. While a Feynman would jump on any opportunity to a solve a practical problem through complicated Feynman diagrams, Bohm would actively resist formalising his intuition, letting the muscle memory of the insight simmer, evolve, and manifest in unexpected ways.\n\n\u003e ‚ÄúDave always arrives at the right conclusions, but his mathematics is terrible. I take it home and find all sorts of errors and then have to spend the night trying to develop the correct proof. But in the end, the result is always exactly the same as the one Dave saw directly‚Äù. - Basil Hiley, Bohm‚Äôs colleague at Birckbeck College\n\nPerhaps all discovery happens this way. In the most messy, ad-hoc, inefficient manner possible, outside the sanitised corporate innovation labs. It is not surprising that when Bohm met Einstein years later, Einstein would confirm Bohm‚Äôs predilections. ‚Äú_Matter_¬†tells¬†_space_¬†how to¬†_curve_, and¬†_curved space_¬†tells¬†_matter_¬†how to move‚Äù, the basic principle behind his general theory of gravity, Einstein confirmed, was not arrived at by years of objective, rational thought. It was an¬†_irrational feeling_, a series of subtle, internal muscular sensations about four-dimensional space-time that the world didn‚Äôt yet have the mathematics to describe.\n\nThis aspect of discovery doesn‚Äôt get discussed in polite company because it is impolite to attribute the profound to the irrational. There are always more convenient explanations fit for the industrialised age ‚Äì like IQ, degrees, and the act of being European. How else does one justify the bizarre amnesia of high-school syllabi? Einstein‚Äôs equations explained the abnormalities in the behaviour of gravity, and predicted blackholes, and gravity waves ‚Äì but this came much later. The muscular intuition came first. It is merely human to rationalise irrationality in hindsight.\n\nThat said, this essay is not about how Einstein sympathised with Bohm, or vice versa. It is rather about the exact opposite.\n\n**Feeling Causality**\n\nGary Marcus, a vocal critic of the currently most popular approach to artificial intelligence ‚Äì deep learning ‚Äì¬†[wrote](https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html)¬†in The New York Times recently about how computer systems need to understand time, space and causality to be more..well, intelligent. What he means is that instead of learning by repetition that anything with two eyes, two ears and whiskers is a cat, it should learn that mammals have these features and hence anything with those features is a mammal, which breathes, excretes and reproduces, and a cat is just a special instance of such an mammal. Deep learning doesn‚Äôt posit an artificial intelligence to understand what¬†_causes_¬†a cat to be a cat, and hence it is brittle, narrow and quite dumb.\n\nDo human beings have intuitive understanding of how causality works? David Bohm would disagree. We have an explicitly¬†_local_¬†understanding of how causality works. In everyday speech, ‚Äúlocality‚Äù is a slightly pretentious word for a neighbourhood, town, or other place. But its original meaning, dating to the seventeenth century, is about the very concept of ‚Äúplace.‚Äù It means that everything has a place, a location. You can always point to an object and say, ‚ÄúHere it is.‚Äù If you can‚Äôt, that thing must not really exist. At any point we can localise the different features of a cat ‚Äì it has ears, eyes, whiskers, and a tail. So we would want our AI to do this intuitively too.\n\nIt is one of the strongest intuitions of the human experience ‚Äì at any point of time, we have a strong sense of place, and of the relation between spaces. We feel a strong sense of separation from those we love, and a particular kind of impotence at being unable to affect things far away.\n\nThe first aspect of this is ‚Äúseparability‚Äù. You can take four china plates and put each one in a different corner of the dining table. They will not cease to exist or lose any of their features‚Äîsize, style, rigidity. The entire dining table derives its properties from the set of plates that make it up; if one hamburger can be put on each plate, then a set of four plates can serve four hamburgers. The whole is the sum of its parts.\n\nThe second aspect of this intuition is ‚Äúlocal action,‚Äù which says that objects interact only by banging into one another or recruiting some middleman to bridge the gap between them. Whenever a distance separates us from someone, we know we cannot have any effect on that person unless we cross the distance and touch, talk to, punch‚Äîsomehow, make direct contact with‚Äîthat person, or send someone or something to do it for us. Modern technology does not evade this principle; it merely recruits new intermediaries. A phone translates sound waves into electrical signals or radio waves that travel through wires or open space and then get translated back into sound on the other end.\n\nLocality was the foundation stone on which Einstein built his castle. His theory of relativity formalises the locality we intuitively take for granted. Per Einstein, the speed of light is the cosmic limit on the speed of causality. Nothing occurs at infinite speed, and hence the world does not start and end at the same time. Places and things are ‚Äúseparate‚Äù, and interact via ‚Äúlocal action‚Äù, limited by this speed limit.\n\nWhat, then, would be the opposite of locality? So convincing is locality, that it‚Äôs opposite is simply referred to with the prefix ‚Äúnon‚Äù : non-locality. In the non-local picture of the universe, nothing is separate. Every particle interacts instantly with every other particle. There exists no concept of time ‚Äì the universe is created and ends at the same frozen instant of time. Similarly there exists no concept of space ‚Äì nothing can be separated, and while the whole may be a sum of it‚Äôs parts, there is no way for someone who is a part of the universe to localise the parts from the whole. As the joke goes, while a stitch in time would just confuse Einstein, a non-local universe would be his worst nightmare.\n\nDavid Bohm discovered a model of the universe consistent with all experiments, that is¬†_explicitly non-local._¬†In a book he co-wrote a few years before his death, he called it the¬†_Undivided Universe_.\n\n**Feeling Non-Locality**\n\nWe shall indulge ourselves in the absolute minutiae of math to understand what Bohmian‚Äôs idea of mechanics says. Like most good ideas, it is so horribly simple, and yet so counter-intuitive that it‚Äôs philosophical meaning has completely eluded the mainstream.\n\nAny theory of the universe can be described by how a state of set of particles is defined and how it evolves over time. In Bohmian mechanics, the state of a system of N particles (q1, q2, q3‚Ä¶qN) is described by its wave function œà=œà(**_q_**1,‚Ä¶,**_q_**N)=œà(q) . The theory is then defined by two evolution equations: Schr√∂dinger‚Äôs original equation that is basis of all of quantum mechanics:\n\n[\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1763d1b9-5f40-49d8-bb6b-2171510787ea_121x72.png)\n\n\n\n](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1763d1b9-5f40-49d8-bb6b-2171510787ea_121x72.png)\n\nAnd a secondary¬†**Guiding equation**, that Bohm called the pilot wave equation, that doesn‚Äôt get included any of the QM textbooks:\n\n[\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5b1119a-f27b-4ad6-a561-8ccd628877aa_274x70.png)\n\n\n\n](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5b1119a-f27b-4ad6-a561-8ccd628877aa_274x70.png)\n\nWhy does nobody like Bohmian mechanics? It‚Äôs predictions match up with all the other more popular interpretations of quantum mechanics, so there is no empirical reason to reject it. Most physicists say it‚Äôs because the guiding equation adds unnecessary complexity to the already elegant Schr√∂dinger‚Äôs equation, hence it must not be required. The real reason is found in the outrageous meaning the equation makes explicit. If you look carefully, the trajectory of any particle¬†**‚ÄúQ_k‚Äù**, is dependent on all other particles in the system¬†**(Q_1‚Ä¶..Q_N)**, mandated by the very simultaneity of the equality sign. No time-delayed dependence, as Einstein would like it. A speed limit isn‚Äôt even required.¬†_Explicit non-locality._¬†This dangerous idea goes against every established idea in contemporary physics, and hence amounts to sacrilege, even though it is effective. This is what the above equation when applied to set of particles passing through Young‚Äôs infamous double split experiment looks like:\n\nWhat the simulation in the video shows is the exact trajectories of a set of particles passing through either of two slits. The idea of determinate trajectories alone is alien to the world of mainstream quantum physics. Bohm‚Äôs equation further predicts how a quantum state deterministically collapses into a classical state. When mainstream quantum mechanics says that the choice of which slit the particle goes through is¬†_fundamentally_¬†random, what Bohm says is that nothing is random ‚Äì if only we knew the initial position and current position of all particles in the universe, we could simply plug it into his equation to find which slit the particle would go through.\n\nThe youtube channel Veritasium, has done us all a brilliant service by creating this video about how to visualise the guiding (pilot) wave equation :\n\nThere‚Äôs another way to intuit Bohm‚Äôs notion of an undivided universe : un-mixing drops of ink in viscous corn syrup. A hollow glass cylinder is kept inside another, and the space between is filled with viscous fluid ‚Äì corn syrup. Three drops of ink are injected into the corn syrup, in a way they appear separate (‚Äúlocalised‚Äù) from each other. On rotating the inner cylinder, the viscous corn syrup moves in layers, layers of liquid farther away from moving cylinder stay static, the ones near it move rapidly. As a result the drops¬†_appear_¬†to mix ‚Äì they go from a state of less entropy to more entropy. They coalesce into a single colour, in a way it would be impossible to separate them in a traditional liquid. But corn syrup is viscous, hence on turning back the inner cylinder, the drops magically¬†_un-mix_, and return to their initial localised positions. Here‚Äôs a video showing how this happens:\n\nThe experiment is just an analogy, but it resembles how David Bohm thought about the universe. In the experiment, there exists what Bohm calls an ‚Äúimplicate order‚Äù (the order of colours of the separate drops) that over time evolves and mixes into this non-local mixture (where nothing is separate, everything affect everything else). Nothing in the mixture appears separate, yet it is ‚Äì if the universes trajectory was rolled back, it would separate into it‚Äôs constituents in a predictable, determinate manner to reveal the ‚Äúimplicate order‚Äù of all the particles since an initial Big Bang event. The order Bohm speaks of is timeless, exists all the time, yet is not apparent.\n\nIn Bohm‚Äôs view, all the separate objects, entities, structures, and events in the visible or explicate world around us are relatively autonomous, stable, and temporary ‚Äúsub-totalities‚Äù derived from a deeper, implicate order of unbroken wholeness. Bohm gives the analogy of a flowing stream:\n\n\u003e On this stream, one may see an ever-changing pattern of vortices, ripples, waves, splashes, etc., which evidently have no independent existence as such. Rather, they are abstracted from the flowing movement, arising and vanishing in the total process of the flow. Such transitory subsistence as may be possessed by these abstracted forms implies only a relative independence or autonomy of behaviour, rather than absolutely independent existence as ultimate substances.\n\n\u003e (David Bohm, Routledge \u0026 Kegan Paul, London, Boston, 1980, p. 48.),¬†_Wholeness and the Implicate Order_\n\nAnother metaphor Bohm uses to illustrate this implicate order is that of the hologram. To make a hologram a laser light is split into two beams, one of which is reflected off an object onto a photographic plate where it interferes with the second beam. The complex swirls of the interference pattern recorded on the photographic plate appear meaningless and disordered to the naked eye. But like the ink drop dispersed in the corn syrup, the pattern possesses a hidden or¬†_enfolde_d order, for when illuminated with laser light it produces a three-dimensional image of the original object, which can be viewed from any angle. A remarkable feature of a hologram is that if a holographic film is cut into pieces, each piece produces an image of the whole object, though the smaller the piece the hazier the image. Clearly the form and structure of the entire object are encoded within each region of the photographic record. Bohm suggests that the whole universe can be thought of as a kind of giant, flowing hologram in which a total order is contained, in some implicit sense, in each region of space and time. When Chinese billionaire Jack Ma recently gifted a hologram gift to his employees, he would have liked to have this notion in mind.\n\nIn perhaps the most important result in the history of quantum mechanics , John Bell in the 1960s theoretically proved through his infamous¬†_Bell‚Äôs inequality_¬†that one could not accept the findings of quantum mechanics without accepting the inevitability of non-local communication. He owed his proof to a singular man ‚Äì David Bohm.\n\n\u003e **‚Ä¶ conventional formulations of quantum theory, and of quantum field theory in particular, are unprofessionally vague and ambiguous. Professional theoretical physicists ought to be able to do better. Bohm has shown us a way.**\n\n\u003e J.S. Bell,¬†_Speakable and Unspeakable in Quantum Mechanics_\n\n**SimulatingNon-locality**\n\nContrary to the misgivings mainstream physics has for the explicit non-locality of Bohmian mechanics, it is used a lot in one particular domain : visualising quantum mechanics via simulation. Everything from quantum tunnelling, to decoherence and time-dependent scattering, is easily plotted and evolved using perfectly deterministic classical computers.¬†[This](https://www.youtube.com/user/kvb100b/videos)¬†youtube channel explores a lot of these in multicolour detail.\n\nWhy is Bohmian mechanics so useful in computer simulations of quantum mechanics? Because while non-locality might seem strange to physicists, it is not as weird to computer scientists. The scientific question of non-locality is, ‚ÄúHow can two particles separated by half a universe be understood as connected such that they interact as though they were right on top of each other?‚Äù. If we analogise to a computer, the question would be, ‚ÄúHow can two pictures at the far corners of the screen be understood as connected such that the distance between them is irrelevant?‚Äù\n\nIn fact, the measured distance between any two pixels (dots) on the monitor‚Äôs display turns out to be entirely irrelevant, since both are merely the products of calculations carried out in the bowels of the computer as directed by the programming. The pixels may be as widely separated as you like, but the programming generating them is forever embedded in the computer‚Äôs memory in such a way that the very concept of separation in space and time of the pixels has no meaning whatsoever for the stored information, similar to Bohm‚Äôs notion of non-locality. To those familiar with programming, written in pseudo code, the guiding equation we encountered earlier looks like any other computer function:\n\n```\ntype particle {  \n   V velocity    \n   X position \n}  \n\nfunction GuidingEquation(Q_1, Q_2, Q_3....Q_N) (Q_K particle) {            \n   ...      \n   return Q_K \n\n}\n```\n\nThe above function takes in the position and velocities of ALL the particles in the computer system, and returns the position of the k-th particle. It is almost trivial to understand how the universe would evolve if it was guided by an equation like this, as opposed to something which drew explicit difference between interdependencies based on a speed limit. Whether the universe is a computer simulation or not is the unfalsifiable subject of another debate, but it certainly gives us another intuition to why non-locality may not be as strange as it appears.\n\n**Echoes From a Forgotten Past**\n\nNon-locality forces upon us an universe that by nature is undivided ‚Äì but the concept of an undivided universe is not in itself new. Cause and effect have been important topics in all ancient schools of Indian philosophy, particularly Vedanta. All schools of Vedanta subscribe to the theory of¬†_SatkƒÅryavƒÅda_, meaning that the effect is pre-existent in the cause. Alan Watts, a prominent Eastern philosopher describes in detail the intricate concept of¬†**_Indra‚Äôs Net_**:\n\n\u003e Imagine a multidimensional spider‚Äôs web in the early morning covered with dew drops. And every dew drop contains the reflection of all the other dew drops. And, in each reflected dew drop, the reflections of all the other dew drops in that reflection. And so ad infinitum. That is the Buddhist conception of the universe in an image. - Alan Watts\n\nThe metaphor of Indra‚Äôs Net originates from the¬†_Atharva Veda_¬†(one of the four Vedas), which likens the universe to a woven net. The net is described as being infinite, spreading in all directions with no beginning or end. At each node of the net is a jewel, so arranged that every jewel reflects all the other jewels ‚Äì similar to Bohm‚Äôs hologram metaphor. No jewel exists by itself independently of the rest. Everything is related to everything else; nothing is isolated. This net describes the universe as a web of connections and interdependencies among all its members, wherein every member is both a manifestation of the whole and inseparable from the whole.\n\nOther texts relate this to the idea of¬†_Brahman_¬†(not to be confused with¬†_brahmin_, which is a caste identity). Even seemingly disparate elements are described as nothing other than reflections of this¬†_Brahman_, and hence of one another. This notion of an organic unity ‚Äì a non-locality ‚Äì is a signature of Indian and most of Eastern philosophy, and distinguishes it from all major Western philosophies ‚Äì that are explicitly local in nature.\n\nVidyƒÅranya in his Panchadasi (V.4) explains the meaning of the phrase, or¬†_mahavakya_, ‚Äú_Aham Brahmasmi_‚Äú:\n\n\u003e Infinite by nature, the Supreme Self is described here by the word¬†_Brahman_¬†(lit. ever expanding; the ultimate reality); the word¬†_asmi_¬†denotes the identity of¬†_aham_¬†and¬†_Brahman_. Therefore, (the meaning of the expression¬†_Aham Brahmasmi_¬†is) ‚ÄúI am Brahman‚Äù.\n\nBohm was astute enough to know the difference between science and¬†_scientism_. A communist who was literally and figuratively ostracised from America during the Cold War, and from the physics community at large, he knew that physicists harboured as much dogma as the mystic shamans. It is no surprise, that the technique of Bohmian Dialogue ‚Äì which acknowledged that rigorous science and schools of philosophy both have similar goals ‚Äì to arrive at an understanding of thought and reality. And so there must be a dialogue in a civilised space, reconciling differences in the absent of any judgement. His dialogues with his collaborator, Jiddu Krishnamurti, an Indian philosopher, have filled many books.\n\nThere is however one man‚Äôs intuition which Bohm‚Äôs notions of locality have never been reconciled with till date. A theory that was as popular as Bohm‚Äôs ideas were unpopular ‚Äì Einstein‚Äôs theory of relativity. Explicit locality. Even today, the many bastions of mainstream physics reject Bohm on these grounds, as if their own vague theories of quantum mechanics are somehow perfectly consistent with relativity.\n\n**Bringing Back the Absolute Frame**\n\nEinstein‚Äôs theory of relativity says that the laws of physics work the same everywhere, regardless of the frame of reference i.e. (1) No frame of reference is privileged in relation to all other frames. (2) All frames of reference are privileged in relation only to themselves.\n\nPrior to Einstein, most scientists believed in a ‚Äúluminiferous ether‚Äù, posited by Lorentz‚Äôs ether theory. The ether was an unobservable all-pervading absolute frame of reference, such that time was definitive only when at rest in that ‚Äúether‚Äù rest frame. Time in this rest frame was the yardstick that time in all other frames in motion would be relative to. They assumed that as beings on the surface of the earth traveling in the ether rest frame , our timing measurements must be distorted, although it was not possible to know by how much. Einstein‚Äôs contribution was to note that even if there were an invisible ether rest frame that we were traveling through, we could still define our inertial rest frame to have all the characteristics of a ether rest frame so that¬†_our_¬†timing measurements were definitive while timing measurement made at rest in the elusive ether would be distorted. In other words, it didn‚Äôt matter whether there really was an ether or not, we could define our own inertial rest frame and everything would work out just fine. In fact the idea of ‚ÄúLorentzian tranformations‚Äù used by Einstein to transform event in one frame of reference to another, is borrowed from Lorentz‚Äôs ether theory, which required there to exist an absolute frame of reference in the first place.\n\nIn short, Einstein‚Äôs approach was decidedly positivist ‚Äì if scientists with PhDs couldn‚Äôt observe the ether, then it meant that it did not exist, and hence wasn‚Äôt worth bothering about. Lorentz ether theory with an absolute rest frame made the same predictions as Einstein‚Äôs special theory of relativity, it was merely another interpretation of the phenomenon described by SR. In truth, there is nothing that prevents the existence of an absolute frame of reference, except Einstein‚Äôs postulates that explicitly say otherwise. The luminiferous ether hence was as quickly forgotten as Bohm‚Äôs ideas about mechanics.\n\nIn no matter of coincidence, the fundamental reason why Bohm‚Äôs notion of reality is inconsistent with Einstein‚Äôs relativity is due to the necessity of faster-than-light propagation i.e. non-locality inevitably picks out a preferred frame. The guiding wave described by Bohm‚Äôs guiding equation works non-locally in this absolute frame. This explicitly violates Einstein‚Äôs postulates. A postulate however, it should be noted, is not a divine truth, but merely an arrangement of words. There are those who believe rabidly in postulates given by men, but can an appeal to an authority that starts with an E instead of a G be really called the scientific method?\n\nCould a neo-Lorentzian ether theory of relativity that explains gravity via an absolute frame, coupled with Bohmian mechanics result in a unified theory of everything? Perhaps. It would certainly do away with the locality-non-locality debate. But we would first need to reject every epistemological notion about physics we hold, and certainly those postulates handed down by science‚Äôs favourite neighbourhood genius, Albert Einstein. If that isn‚Äôt a controversial idea, then maybe 9/11 was indeed an inside job.\n\n**A Delicate House Of Cards**\n\nWhile critiquing Einstein is easy, it is also easy to forget that he was the first to notice the inconsistency. In his famous paper describing what is known today as the EPR paradox, he would list his problems with the inherent non-locality in quantum mechanics, or what he called ‚Äúspooky-action-at-a-distance‚Äù. In so many words, his message was clear : ‚ÄúMy intuition tells me this is wrong, and hence it is wrong(or incomplete)‚Äù.\n\nB.R. Ambedkar, the composer of the Indian constitution was Mahatma Gandhi‚Äôs anti-christ. He advocated nuclearisation, industrialisation, militarisation and the abolishment of the caste system as the only way to develop the nation. Meanwhile, the Mahatma championed a rural aesthetic, the theatre of non-violence, and simple self-sustained living in the style of his many ashrams. Even today, many appropriate Gandhi to serve their own means, but looking around, India has largely gone down the way professed by Ambedkar. Both were men of powerful intuition about what¬†_felt_¬†right. Both are villains and heroes of different fan followings, but Ambedkar is mentioned in the same breath as Gandhi precisely because his intuitions were as powerful as Gandhi‚Äôs. As Leo Tolstoy would put it ‚Äì ‚ÄúThe best stories are not between the good and the bad, but between the good and the good.‚Äù\n\nAlbert Einstein‚Äôs intuition, the castle in the air that put locality above everything else, will perhaps attain a similar measure of piousness that Gandhi has. Bohm‚Äôs ideas too have mostly stayed outside the mainstream for mainly epistemological reasons, relegated to the elephant graveyard of physics, trying to make their way in. It is a crime of human bias, as John Bell puts it:\n\n\u003e Bohm‚Äôs 1952 papers on quantum mechanics were for me a revelation. The elimination of indeterminism was very striking. But more important, it seemed to me, was the elimination of any need for a vague division of the world into ‚Äúsystem‚Äù on the one hand, and ‚Äúapparatus‚Äù or ‚Äúobserver‚Äù on the other. I have always felt since that people who have not grasped the ideas of those papers ‚Ä¶ and unfortunately they remain the majority ‚Ä¶ are handicapped in any discussion of the meaning of quantum mechanics. Why is the pilot wave picture ignored in textbooks? Should it not be taught, not as the only way, but as an antidote to the prevailing complacency? To show that vagueness, subjectivity, and indeterminism are not forced on us by experimental facts, but by deliberate theoretical choice? - John Bell\n\nWhat happens to men who are firmly set in muscled intuition when they come across something counter-intuitive? It burns. Like a immune system violently responding to a pathogen. Those who‚Äôve moved past science‚Äôs most loved high-school myth, that electrons move around a sun-like nucleus in planet-like orbits have come across the usual suspects of quantum mechanics ‚Äì Bohr, Schr√∂dinger, Heisenberg et al. None of them noticed this as much as the one person whom it burned the most ‚Äì Albert Einstein. He was hence the first to point it out. The first real threat to his castle in the air. David Bohm merely aggravated it from an itch to a burn.\n\nIt takes one powerful intuition to counter another. Which is perhaps why we should listen to the most dangerous ones.","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/causality":{"title":"The Unsettling Physics of Causality","content":"A general relativity theorist, Bert (GR) and a quantum mechanist, Bomm (QM), find themselves in a bar that‚Äôs about to close down.\n\n**Bartender**: I will allow you to talk aloud about physics, but it‚Äôs late, I‚Äôm tired, and if I have to listen to you both ramble on at 12 midnight, you have to leave out all the math.\n\n**Bert (GR)**¬†: Blasphemous! Understanding general relativity without Einstein‚Äôs equations is like smelling fine wine without tasting it. The beauty, the simplicity‚Ä¶\n\n**Bomm (QM)**¬†: And quantum mechanics doesn‚Äôt make sense outside of Schr√∂dinger‚Äôs equations. The¬†_wavefunction_¬†is all that is real.\n\n**Bartender:**¬†Ok, say the world is large video game working on a computer. How do I get from point A to B inside the game?\n\n**Bert (GR)**¬†: That would depend on how far point A and B are. And how fast you‚Äôre travelling.\n\n**Bartender**¬†: It‚Äôs a video game. I should be able to get from point A to B at infinite speeds.\n\n**Bert (GR)**¬†: That is impossible, of course. Everything in the universe video game must have a speed upper limit. Otherwise, if things happened at infinite speeds, all observable events would happen simultaneously. There would be nothing separating cause and effect. No concept of time. All causes and and all effects would happen at once. The video game would start, and end, at the same time.\n\n**Bartender**: Makes sense. So what‚Äôs our universe‚Äôs speed upper limit?\n\n**Bert (GR)**¬†: The speed of light. 300,000 km/s. That‚Äôs around a million times the speed of an airplane, which is why it might seem infinitely fast, but isn‚Äôt. Physics is all about causal interactions. A bullet leaves a gun and hits a person, the person then dies. The person can‚Äôt die BEFORE the bullet hits him. Cause and effect. The speed of light, hence, is the speed of causality ‚Äì i.e. the upper limit to the speed at which things happen.\n\n**Bartender**¬†: Ah. If I‚Äôm driving on the highway and there is car on the other road travelling towards me, I see it travelling at twice my speed. If my car could travel at the speed of light, I would see the opposite car travel at twice the speed of light. Doesn‚Äôt that violate your upper limit?\n\n**Bert (GM)**¬†: Ah but you see, that‚Äôs where the universe is clever in how it works. There is no absolute time, or length, or velocity. i.e. there is no absolute, fixed order of causes and effects distributed in time. It‚Äôs all¬†_**relative**_¬†to the speed at which you‚Äôre travelling i.e. your frame of reference.\n\n**Bartender:**¬†I‚Äôve heard of this one. The theory of relativity.\n\n**Bert (GM)**¬†: So if two cars were travelling towards each other at the speed of light, time would slow down for the person sitting in both those cars, so that the order of observable events would always be restricted to the speed of light. Time would grow, and distance travelled would shrink, so the velocity (which is distance divided by time) of the two cars would appear to contract. So that if the two velocities were added, it would always be less than the speed of light. Since the ordering of cause and effect is relative to the speed at which you‚Äôre travelling, all participants ‚Äì the two drivers, and also an observer standing on the side of the road ‚Äì can agree on the order of observable events, even though their individual clocks don‚Äôt match. In no case, or, in no frame of reference, can a person die, before a bullet hits him.\n\n**Bartender:**¬†So it‚Äôs a hack to prevent causal conflicts in the world. I could live with that.\n\n**Bert (GM):**¬†Well, crudely put, yes. We physicists call this¬†_**locality**_¬†‚Äì local causal relativity. Local cause and effects mean cause and effect separated by the maximum speed of causality. All of Einstein‚Äôs laws of how planets move, how black holes work, how gravity works, arise from this fundamental sacred truth. General relativity, too, simply describes the geometry of causality.\n\n**Bartender:**¬†I notice Bomm here has been quiet all along. Looks like he isn‚Äôt too happy with this explanation. What‚Äôs wrong?\n\n**Bomm (QM)**¬†: Quantum mechanics is at odds with this idea. Not only is it NOT local, it is, quite unsettlingly so,¬†_explicitly non-local._\n\n**Bartender:**¬†You mean to say that causality works faster than light? At infinite speeds?\n\n**Bomm (QM):**¬†Yes. Macroscopic objects we see around us seem to have one fixed property at a time. This tissue is white, this whisky is black . Objects at microscopic level behave differently. When isolated and until observed, they exist as a superposition of two or more possibilities ‚Äì say State Black and State White, at the same time. They collapse into one when you observe them. When you bring two of them together ‚Äì they entangle in an inseparable way. So inseparable, that if you took these two to opposite ends of the universe, separated them by lightyears, and then measured one of them, it would collapse into one of the possibilities. Say you observed Black. You would then see that the other atom would be White. Or if one was White, other would be in Black. Everytime. The atoms would seem to have conspired from across the universe to be exactly correlated, or anti-correlated. Spooky action-at-a-distance. Cause and effect separated by seemingly infinite speeds. Laboratory experiments have pinned this to 50,000 times the speed of light, but it could be much much higher. Infinite.\n\n**Bartender:**¬†Perhaps the atoms could have conspired beforehand about which state they would collapse into, and we just can‚Äôt see how they did so?\n\n**Bomm (QM)**¬†: Ok let‚Äôs run an experiment. Say I put you and Bert in a room, and told you I would ask you both three Yes-No questions, A, B, \u0026 C separately when you came out. There are two ways you could conspire to correlate answers for all 3 without me finding out ‚Äì First way, you could decide before hand to always answer Yes, or No, or some combination. For 3 questions, that is 2^3, or 8 combinations. Or the Second way ‚Äì when I ask you the question, you could call each other and decide on the answers you could give. Is there any other way you can think of in which to get exactly correlated answers to all three questions?\n\n**Bartender**¬†: No, I don‚Äôt think so.\n\n**Bomm (QM)**¬†: So as you notice here, both the ways are strictly¬†_local_¬†‚Äì whether you conspire beforehand, or call each other at the time of asking, cause and effect are always separated by the speed of causality. No instantaneous communication happens at the moment I ask the questions. One could similarly think that the atoms too could have such local communication via methods hidden from us, and Yes/No answers to be Black/White. Still with me?\n\n**Bartender**¬†: I think so.\n\nBomm takes out a napkin and writes down all the possibilities for the answers to the questions.\n\n**Bomm (QM)**¬†: Our implicit assumption here, is locality. If we indeed do assume the first way, here are all the 8 possible combinations you could have conspired to beforehand. The fact is then, that in each pre-determined conspiracy, there are at least two questions with the same answer. If we repeated this experiment a million times, would you not say then that if we added up the frequencies of the answers being same to any two questions, the sum would be greater to or equal to one? That you would at least be assured that any two questions have the same answer?\n\n**Bartender**¬†: Half of the possible combinations has at least two answers same, so I‚Äôd say half for any two questions chosen. And since there are three ways to choose any two questions, the maximum would be three times half, or 3/2, and the minimum would 1.\n\n**Bomm**¬†(QM): Correct! But experimental evidence seems to contradict this obvious result. If we repeated this experiment a million times with a pair of entangled atoms, and added up the frequencies of the answers being same to any two questions, the sum comes to hover around 3/4. Which is less than 1. Not greater than or equal to one, as our mathematical description of pre-determined strategies necessitates. A contradiction!\n\n**Bartender**¬†: Which means there is no way that the answers could have been pre-determined, no hidden conspiracy could have made them correlate their states exact with respect to the other?\n\n**Bomm (QM)**¬†: Precisely! And here lies the problem. The hidden pre-determined conspiracy in physics is called a theory of¬†_**local hidden variables**_. General relativity may wear out, so might quantum theory, but there is no way to escape this statistical contradiction, this..this inequality, as John Bell put it. As long as 3/4 is not greater than or equal 1, there can be¬†_**nolocal hidden variable theories**_. Since locality was our only assumption, and it has been disproved, we must assume non-locality.\n\n**Bartender**: So at the moment the question is asked, the atom instantaneously communicates with it‚Äôs entangled pair, faster than the speed of light? Does that mean we can use this to boost internet speeds? Download things faster than the speed of light?\n\n**Bomm (QM)**¬†: No, I‚Äôm afraid not. While the two entangled atoms are indeed correlated across huge distances, which state one of them would collapse into is completely random. You can‚Äôt control which state one of them collapses into, and so you can‚Äôt control the other‚Äôs state. Information is about controlling the flow of 0s and 1s. Here, there is no way you can control the transfer of information at speeds faster than that of light ‚Äì it would be like sending garbled random noise from one place to another. Useless.\n\n**Bartender :**¬†Ah so from what I understand, we have one theory of physics where everything happens at the speed of light, and another where everything happens at once. My car‚Äôs gear train is a local mechanism. Motion passes from one gear wheel to another in an unbroken chain. Break the chain by taking out a single gear and the movement cannot continue. Without something there to mediate it, a local interaction cannot cross a gap. On the other hand, the essence of non locality is unmediated action-at-a-distance. A non-local interaction jumps from body A to body B without touching anything in between. Like a voodoo doll injury.\n\n**Bomm (QM)**¬†: You‚Äôre picking things up really fast!\n\n**Bartender**: However, it seems like both of you deal with difficult scales ‚Äì Bert deals with large objects, and Bomm with really small ones. There‚Äôs just different laws for each. What‚Äôs to be in conflict about?\n\n**Bert (GR) :**¬†Because none of these theories put a theoretical scale at which the respective theory stops working. There is no defined line between the microscopic and the macroscopic.¬†_Theoretically_, quantum mechanics works at ALL scales. So does general relativity. Yet, the universe doesn‚Äôt work that way. Which means both are incomplete. They cannot be reconciled.¬†_Yet._\n\n**Bartender**¬†: How can two models of physics of the world be so diametrically opposite to each other? One of you swears by¬†_locality_¬†and the other can‚Äôt do without¬†_non-locality_.\n\n**Bert**: Pfft, very rich for bartenders to complain about it.\n\n**Bomm:**¬†What do you think brings us here everyday?\n","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/economy":{"title":"On Charkhas, AI \u0026 Economic Growth","content":"\nAlmost everyone gets the question of 'AI economic impact' completely wrong. a few leaps of imagination is all that it takes.\n\nHow does economic disruption work? A new abstraction comes along that renders an existing profession redundant. So in the 1800s, the handloom rendered the textile workers of the South redundant. Cars rendered horse drawn carriages redundant. \n\nThe way technological progress works is that a new technology/method that is 100x better replaces scores of slightly mediocre solutions implemented in parallel. The charkha isnt just an inferior solution, both serially and in parallel, the handloom is just a completely new thing altogether. \n\nTodays discourse is solely in terms of unit-wise replacement. Humanoid robots replacing other blue collar workers. One programming agent replacing another an SWE at work. One AI call center employee replacing 5 call center employees.\n\nOr its about quantity. 100 billion new agentic programmers working at your beck and call! 50000 AI researchers! What would a company do with 500 new employees that worked 24 hours a day, never tired. The Charkha mindset, again. \n\nWe have these incremental ways of thinking about the effects of AI because we dont really have any AI technology that can invent new things. We dont have anything that can look at a charkha, causally model all the parts that work together, evaluate the inputs and outputs in the supply chain, identify bottlenecks which require the most human intervention, and painfully iterate on hardware prototypes until it invents the handloom. If you asked charkha operators in 1700s what machine would replace them, they'd say something that like a machine that spins the wheels faster, or auto-loads the yarn. They would never think of the motorized handloom. \n\nIn todays terms this looks like : 'AI writes 80% of my code'. This is meaningless for any economic miracle. What you want is the kind of AI builds something new that replaces the need for vast swathes of programmers to ever be required to write that code in the first place. \n\nInfact, AIs that can invent and discover are entirely possible to build and will change the way we think about the economic impact of AI. If we assume these machines can invent new things, and plan out entire technology trees of progress by causally chaining parts of economic clockwork in ways where the sum is exponentially better than any the parts, then the way to think about AI is like the role of an inventor. \n\nThe role of an inventor is not just an incremental improvement, it is an entirely different phase change from all other types of economic activity. There is no precedent, no previous training data, no sign that the impossible is within grasp apart from an irrational nudge that says keep going on. 99% of outcomes are failure, and each instance failure has to be modelled uniquely, and mined for just the right lessons to try the next attempt with. One mistake in this modelling, or mistake in measurement and you misattribute failure to the wrong changing variable, or overindex on noise, leading to vicious cycle of failure that is hard to recover from (this cycle is why inventing new things is so rare, and we dont have any set ways to reason about it). Insight and inspiration comes from entirely unexpected places. It is impossible to teach or train for the method of invention because by definition no curriculum includes anything new. Often it involves finding and standing on the shoulders of obscure giants forgotten in time due to epitemological reasons. Often, leaps of faith in the face of heaps of counterevidence. The discovery of fire was an irrational act. We are descendants of the most irrational ape.\n\nBut when it works, the messy process of invention deals wonders. It results in an ecological change - a forest without a tiger is just a bunch of trees, a forest with a tiger isnt just the same forest with the addition of a tiger, its a completely different forest altogether. \n\nSo what does economically disruptive AI look like? It doesnt look like 5000 spun up AWS instances running 500 instances of the same agent loop. It doesnt look like 5000 1-person companies catering to the same consumer demand with fewer people. It looks like a team of 4-5 AIs working in solitude, mostly away from noise of daily trends, building a library of tools and methods that build on top of and complement each other, like Newton inventing calculus to explain gravity, iterating quickly and mining the correct signal from failure, incrementally making and validating the subcomponents of their hypotheses, adapting to change. They work to make one expressive abstraction/tool - a new vertically integrated factory that produces 5 electric cars per second during the day, and can be repurposed to manufacture appliances at night. A new type of cooking appliance that takes in ingredients and outputs cooked packed meals, that fits in a room/kitchen top, rendering a lot of restaurants useless. new virtual reality technology that makes in person work feel like co locating, shooting down property prices in city suburbs. A new fusion method that is so ubiquitous and easy to harvest, that it renders most oil geopolitics irrelevant overnight. A medicine that makes it 1000x less energy expensive to produce a female ovum, upending every scarcity-based gender gynamic in society. A way of programming matter so physical goods can be 3d printed in house, reducing the need to ship things all over the world. \n\nIn common discourse, there is a distinction drawn between AIs that can just automate existing tasks vs AIs that can invent new things. I differ here. All economic value comes from inventing/creating distinct abstractions to manipulate the environment - the world puts a higher value on you depending on how unique your abstraction library is. New recruits are valued based on how quickly they acquire new skills (not based on how many things they can do). New companies are based on the tech tree they can own and manipulate, the unique abstraction tree that only they have access to. Abstractions cease to become valuable when they are commoditized - airlines were valuable services when only a few knew how to make planes. But today in aviation only boeing and airbus make money by selling planes to everyone. They have figured a proprietary abstraction ladder that only they have tools, personnel, institutional scarring and experience with to maintain and use. AI disruption will work similarly.\n\nThink obscure strange unique abstraction ladders spun up ad hoc over weeks (not years), that allow an AI to build and offer a 1000x better service/product for a 100x lesser price. What does a few cycles of this iterative oneuppance look like? The world looks very different when you have AIs rapidly building on each others abstraction libraries, exchanged via precise context free grammar (and not encumbered by stagnating high school and college curricula). This is the real disruption to worry about because it's non-linear and hard to predict. The handloom replaced an entire manual textile industry, what happens when you have a machine that can climb the abstraction ladder to create arbitrary handlooms for arbitrary industries at arbitrarily short time scales? ","lastmodified":"2025-03-04T06:29:26.518901425Z","tags":[]},"/notes/nd":{"title":"non-duality","content":"The best way to understand the mind, is to build one.\n\nThe best way to understand the universe, is to build one.\n\nBut to the non-dualist, there is no duality of purpose.\n\nTo understand one, is to understand the other.\n\nTo build one, is to build the other.\n\n\n \n\n\n","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/nd/purpose":{"title":"purpose","content":"The best way to understand the mind, is to build one.\n\nThe best way to understand the universe, is to build one.\n\nBut to the non-dualist, there is no duality of purpose.\n\nTo understand one, is to understand the other.\n\nTo build one, is to build the other.\n\n\n \n\n\n","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/no-secrets":{"title":"This Magic Trick Has No Secrets","content":"**Ed:**¬†Oh great shaman, I‚Äôve traveled far to the Himalayas to witness your magic trick.\n\n**Sh:**¬†Yes, I will show you it now. It consists of three tasks. You belong to the city ‚Äì it would seem you people have short attention spans, so I will be quick. But first, take off your shoes.\n\nEd takes off his shoes and the shaman takes out two boxes. He proceeds to tie both the shoes together by their shoelaces, ensuring they don‚Äôt come apart.\n\n**Sh**¬†: Now wear this hood while I prepare this magic trick.\n\nEd obliges. After a while, he takes it off and sees the two boxes in from of him. His shoes are gone.\n\n**Sh:**¬†Your first task is to ask a question to figure out which box has both the shoes .\n\n**Ed (scratching head)**¬†: Umm, which box has both the shoes together?\n\nThe shaman opens the left box. It has both shoes tied together. He repeats this a few times ‚Äì hooding and dehooding Ed between experiments. Results are as expected.\n\n**Ed:**¬†This does not seem to me a very impressive magic trick. You tied my shoes together and just shift it from box to box when my hood is on.\n\nThe shaman hoods him again. He unties the shoes and separates them into two.\n\n**Sh:**¬†Ok then the second task ‚Äì ask a question to figure out which box has the right shoe and which the left.\n\n**Ed (getting flustered)**: This is ridic‚Ä¶okay which one has the left shoe and which one the right?\n\nThe shaman opens both the boxes to show him what‚Äôs inside each. Results are as expected.\n\n**Sh:**¬†We can repeat this a few times if you‚Äôre not convinced.\n\n**Ed (getting up):**¬†Oh I‚Äôm convinced all right. I‚Äôm convinced that this was a massive waste of time.\n\nThe shaman, nonplussed, hoods him again.\n\n**Sh:**¬†Now the third task. Please remove your hood and ask a question.\n\n**Ed:**¬†Which question should I ask?\n\n**Sh:**¬†My friend, now you know both questions, you must choose.\n\n**Ed:**¬†Ok, in which box are my shoes together?\n\nThe shaman opens both the boxes to show both the shoes in one of them, tied together, the other box as empty.\n\n**Ed (a bit puzzled):**¬†What if I‚Äôd chosen another question?\n\n**Sh:**¬†Why don‚Äôt you try?\n\nHooding and dehooding happens.\n\n**Ed:**¬†Which box has the left shoe and which the right?\n\nThe shaman opens both the boxes, showing that each shoe is now in one of each box, now untied.\n\n**Ed (befuddled)**: How did that happen? How did you know which question I was going to ask?\n\nThe shaman shakes his head.\n\n**Sh:**¬†I didn‚Äôt. Whichever question you choose to ask, is appropriately answered.\n\nPerplexed, Ed tries this third experiment multiple times. Every time, he sees a result appropriate to the question he asked but inappropriate to the other question he could‚Äôve asked.\n\n**Ed:**¬†Surely this is unreasonable. The shoes had to be either tied together or separated right before I asked the question? What‚Äôs the trick? Hidden assistant? Some hidden compartment in the box‚Ä¶\n\nThe shaman takes him inside a large cold refrigerated room. Inside is a photon beam, and two more boxes with holes, kept on top of each other.\n\n**Sh:**¬†Here‚Äôs an atom. Its wavefunction is simultaneously in both those two boxes. A veritable Schrodinger‚Äôs cat, if you will. Do you agree this is possible?\n\n**Ed (shivering):**¬†Obviously.\n\n**Sh:**¬†We now send a photon through the tiny holes in the top box. Were the atom in that box, the photon would bounce off the atom in a new direction. Were the atom in the bottom box, it would go straight through unchanged in direction. What do you think the photon would do?\n\n**Ed:**¬†Since the atom is simultaneously in both boxes, the photon does both the things. The atom‚Äôs wavefunction becomes entangled with that of the photon. The parts of the atom‚Äôs wavefunction in each box would no longer by themselves be in a superposition; they would have ‚Äúcollapsed.‚Äù I‚Äôve done this in a lab a bazillion times. What point are you trying to make?\n\n**Sh:**¬†What happens next? If the photon does not interact with anything else, a tricky two-body interference experiment with this set of box pairs and photons could demonstrate that the atom was still simultaneously in both boxes ‚Äî and that the photon had both bounced off the atom and gone through an empty box.\n\n**Ed:**¬†Well yes, but this doesn‚Äôt explain the magic trick. I‚Äôm not leaving until you tell me the secret behind it.\n\n**Sh:**¬†Do you not see ‚Äì the result is dependent on what experiment you do! You could just look in one of the boxes, and find the atom in one of the boxes. Or do the interference experiment and find it in both. Despite your training as a physicist, do you still insist that a physical reality exists independent of your conscious observation of it?\n\n**Ed**: But in this quantum experiment, the detector apparatus in interference experiment is doing the ‚Äú_observing_‚Äú, not me. Any_thing_¬†could observe, there is no need for this consciousness mumbo-jumbo.\n\n**Sh:**¬†Ah, but that is too simple an explanation. A photon coming through one of the boxes does not observe whether or not the atom is in that box. It rather joins a superposition state with the atom. Say it moves towards a detector that‚Äôs built to fire if the atom is in the top box, and not fire if it‚Äôs in the bottom one. Should the photon hit the isolated detector, being a physical system, that detector just joins the atom-photon superposition state. The detector is simultaneously fired and unfired, and the atom is still simultaneously in both boxes. Should you get a second measuring apparatus, say your eyes, to observe the detector, it too would join the superposition, for the same reasons ‚Äì and so on and so forth a causal chain would be formed ‚Äì a giant superposition.\n\n**Ed:**¬†Yet, but when I look at the detector, I see a particular result, not a superposition.\n\n**Sh:**¬†Exactly, but how? John von Neumann showed that the mathematics of quantum mechanics allows the collapse of the wave function to be placed at any position in the causal chain from the measurement device to the ‚Äúsubjective perception‚Äù of the human observer. By definition, whatever is at the end of this causal chain must collapse the wavefunction. It should also be non-physical, not satisfying Schrodinger‚Äôs equations or any physical law, otherwise, it‚Äôd also be entangled in the superposition. Some call this ‚Äúconsciousness‚Äù, some call it¬†_ƒÄtman_, you may call it whatever you want. You may also not call it anything at all.\n\n**Ed:**¬†Ok, yes I agree, whether the atom is in superposition or not, depends on my choice of the experiment. What does my own free will specifically have to do with it? The choice of which experiment could be made by robot too. I can program a robot to take random decisions by say‚Ä¶flipping a coin if you will. Heads, it looks in the box; tails, it does the interference experiment.\n\n**Sh:**¬†Agreed, you could do that. Say we had thousand such box-pairs. And say it flipped this coin a thousand times for each pair. 550 times it got head, and so the atom stayed in one of the boxes in the pair. The other 450 times it got tails, so the atom stayed in the superposition. The result, you find, is puzzling ‚Äì the coin‚Äôs landing is mysteriously connected to what is there in the respective box-pair set! The robot‚Äôs supposedly random choice will strangely always be the ‚Äúright‚Äù choice. This would mean that there is some direct dynamic relationship between the coin landing and the observation made ‚Äì you could then conclude that the coin landing is what causes the atom‚Äôs wavefunction to collapse!\n\n**Ed:**¬†Of course that is absurd ‚Äì Bell‚Äôs theorem explicitly forbids that sort of causation. Causal relationships that travel at the speed of light, i.e. local causal relativity can never cause collapse of a wavefunction. Classical mechanics does not cause quantum phenomenon.\n\n**Sh:**¬†Very good! Now, unconvinced by the theory that the landing of the coin causes the wave-function to collapse, you replace the robot‚Äôs decision making with the only decision-making process you know has nothing to with what‚Äôs going in the box-pair. What is that?\n\n**Ed:**_My own free choice?_\n\n**Sh:**¬†Yes! You would attach a button to the robot instructing it which experiment to do. You would direct it‚Äôs actions, it would not have any agency of its own. That brings you back into the causal chain.\n\n**Ed:**¬†So what you‚Äôre saying is the world is willed into existence by my observation of it? A tree doesn‚Äôt exist when I‚Äôm not looking at it?\n\n**Sh:**¬†If there was someway to isolate a tree from local causation, like we can do conveniently for the minuscule atom, then yes it would not exist as anything other than a probability wave. But a tree doesn‚Äôt exist so, and hence your brain perceives it as a series of particles in position and time. The reason we observe only states characterized by unique positions is that we humans are beings who can experience only position (and time). Speed, for instance, is just position at two different times.\n\n**Ed:**¬†So‚Ä¶it might be conceivable that there could be other beings could experience reality differently? Maybe even possibly directly experience the superposition states whose existence we can only infer. I need some water.\n\n**Sh:**¬†Perhaps. We can never know for sure. And it‚Äôs not just me saying it ‚Äì von Neumann, Bohr, Bell, Wheeler, Wigner and a bunch of others. They and I may be wrong, but the facts are in front of you, and it is up to you to decide.\n\n**Ed:**¬†You are asking me to believe that your magic trick is explained by quantum mechanics. Surely there‚Äôs trickery involved. Your demonstration involved more than an expression of conscious intent. It required you to move your hands and open the boxes. Perhaps the mechanical opening of the box pairs, either sequentially or simultaneously, somehow physically untied and put the shoes wholly in a single box or spread them over two boxes.\n\n**Sh**: Ah but no! This magic trick has¬†_no secrets_. Nothing you have seen violates your laws of quantum physics. Quantum mechanics doesn‚Äôt have any restrictions on the size of the physical system. Our ‚Äútechnology‚Äù has merely enabled us to extend to larger objects what your Bohr of Copenhagen has taught you to accept with equanimity for the small.\n\n**Ed:**¬†I would like my shoes back, please.\n\nThe shaman looks at Ed, grinning. They go out of the room. Ed sits down to wear his shoes, bids farewell to the shaman and proceeds to leave.\n\n**THUD**. Ed has collapsed, face first, flat on the ground. The shaman looks back.\n\n**Sh:**¬†Oh, those shoelaces‚Ä¶","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/panini":{"title":"The PƒÅ·πáinian Approach to Compression","content":"\u003e \"The simplest of several competing explanations is likely to be the correct one\" - Occam's Razor\n\n## Intelligence as compression\n\n(Skip to [code](https://sibeshkar.github.io/notes/panini/#p%C4%81%E1%B9%87inis-razor) walkthrough.)\n\nPhilosopher John Searle in his famous [Chinese Room argument](https://plato.stanford.edu/archIves/spr2010/entries/chinese-room/) argues against the possibility of a computer ever being able to think. From outside the closed room it may look like it's conversing in Chinese, but could the machine inside with a memorized table of what Chinese word comes after what be considered to have understood Chinese? The argument intends to show that while suitably programmed computers may appear to converse, they are not capable of developing any understanding of meaning or semantics, even in principle. \n\nSearle's argument is incorrect. We will soon see how.\n\nToday's most prominent AIs, large language models (LLMs) like DeepSeek, GPT, or Claude are more sophisticated versions of such shallow 'generative' memory models. They use thousands of tokens of context and the 'attention' mechanism to focus on relevant parts of the input to 'query' a table with.  They're not *just* storing a memorized table of what comes after what, but using compute to extract and store a hiearchy of reusable information in their layers. A result is that the weights of the Llama-65B model occupy around 365GB on disk, down from the 5.6TB it's trained on **(a 14x compression)**[1]. We see that generalization ability and data efficiency are equivalent: generalization comes from squeezing every bit of information out of your datapoints, 'understanding' all correlations and causations, and connecting all the dots. \"Squeezing every bit of information\" is meant literally: generalization is the very direct result of compression. \n\nOne could argue against calling this process of parroting statistical regularities 'true understanding'. A 14x (or so) compression ratio would be fairly impressive if nothing better existed. However, these machine learning algorithms are many orders-of-magnitude less data efficient than human beings. Lee Sedol, a top Go player, played around 10,000 games in his lifetime, while DeepMind's bot AlphaGo bot required 30 million games to match him (powered by the energy requirements of a small city). If Sedol had played 30 million games, how skilled would he be? What would a human who has absorbed all of human knowledge look like? What sort of \"information squeezing\" is the human brain doing so effectively? I am convinced that the answer to these questions is the key to building general machine intelligence i.e. machines that think, learn, adapt to tasks like (or better than) humans do.\n\nBut it's hard to run this experiment because most humans have seen orders of magnitude less data than any LLM in their lifetimes, and it's hard to manually inspect human priors. Or so I thought, until I attended the lecture series[2] by Dr. Saroja Bhate at Bangalore International Centre, on PƒÅ·πáini (pronounced \"pah-nee-nee\"), the ancient Sanskrit grammarian. Over 2300 years ago, before the advent of computers or formal logic, PƒÅ·πáini sat down and methodically reduced all of human knowledge, then floating around in spoken Vedic Sanskrit, into a generative grammar of exactly 3,995 *s≈´tras*, or rewrite rules - recorded in his magnum opus, the *A·π£·π≠ƒÅdhyƒÅyƒ´*. These rules have remained unchanged ever since. \n\nFaced with a large corpus of spoken Vedic and contemporary Sanskrit, many thousands of hours of audio signals collected without any substrate to record with or automated tooling to work with, PƒÅ·πáini, over 12 years, found abstracted atoms of meaning that when combined with a set of dynamic rules and meta-rules formed a generative grammar, a deterministic state machine that could be used to re-synthesize the original audio corpus - and be recited in just 2 hours (an astonishing compression ratio of atleast ~5000:1)[3]. \n\nHis work is the first formal system known to man, doing to linguistic reality what Euclid would go on to do later for geometry, but it would be no overstatement to recognize it the only example I've seen of true optimal [Solomonoff induction](https://en.m.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference) - finding the shortest unchanging executable archive of a dataset, as evidenced by it's durability over two millenia. \n\nIndeed, the very existence of PƒÅ·πáini and his work disproves Searle's argument. Yes the machine in Searle's room doesn't understand Chinese, but there is someone in the system who does - the hidden compressor that created the rule table for the machine to lookup (a grammarian like PƒÅ·πáini). The compressor applied the dual techniques of abstraction and economy ruthlessly to thousands of noisy signals of various forms and fidelity, reducing them by many orders of magnitude into succinct set of formal predicates - deriving a fixed set of rules, an explanation unchanging in time. Like breaking down a house into basic Lego-like blocks and then building a new house from it back gain, the compressor like this could then combine these discovered reusable concepts on the fly using abstracted transformation rules to generalize to any unknown. Tomorrow if the raw source signals changed, the compressor would have a method to refactor the rules entirely for this new version of reality. The compressor is where the \"understanding\" is, the process of compression is where the true \"intelligence\" in the closed system resides. \n\nWhen we talk about building intelligent machines, it is indeed building this compressor which we must talk about, NOT the fixed-in-time rules it discovers (a hand-written program, or a set of weights in a neural network). A digital superintelligence in action, would very much look like a grammarian on steroids, ruthlessly employing what we describe above as \"PƒÅ·πáini's Razor\". It is the efficient, automated grammarian which is intelligent, not the ever-updating grammar it generated. Of course, PƒÅ·πáini was working with Sanskrit, whose underlying structure makes it less context-sensitive than English and more amenable to such decomposition. But for the sake of comparison, his methods if automated and applied to the [Hutter compression prize](http://prize.hutter1.net/) dataset could compress 1GB of Wikipedia data to a few kilobytes (down from the current record of 110MB as of Feb 2025). \n\nSearle would perhaps not have made the Chinese Room argument had he heard of PƒÅ·πáini's techniques. To build thinking machines of the future, we must not repeat his mistake. \n\n## PƒÅ·πáini's Razor\n\nI have no formal training in Sanskrit, and the following is merely a programmer's attempt to reverse-engineer PƒÅ·πáini's methods of compression. This is largely a guide for other programmers, so we dive into code right away. Let's first look at how generative grammars compress information. All mistakes are mine.\n\nImagine you're trying to send the first 100 Fibonacci numbers to a friend. The naive approach would be to simply transmit these numbers directly. Let's see just how big this sequence gets:\n\n```python\ndef generate_fibonacci(n: int) -\u003e List[int]:\n    \"\"\"Generate first n Fibonacci numbers.\"\"\"\n    sequence = [1, 1]\n    for _ in range(n-2):\n        sequence.append(sequence[-1] + sequence[-2])\n    return sequence\n\nfib_sequence = generate_fibonacci(100)\nprint(\"First 100 Fibonacci numbers:\")\nprint(\", \".join(str(x) for x in fib_sequence[:10]) + \"...\")\nprint(f\"100th Fibonacci number: {fib_sequence[-1]:,}\")\ndirect_bits = sum(math.ceil(math.log2(x)) for x in fib_sequence)\nprint(f\"\\nDirect storage needs {direct_bits:,} bits\")\n```\n```\nFirst 100 Fibonacci numbers:\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\n100th Fibonacci number: 354,224,848,179,261,915,075\n\nDirect storage needs 2,649 bits\n```\n\nThat's a lot of information! The numbers get very large very quickly - the 100th Fibonacci number is a 21-digit number. Let's see how different approaches to compression handle this much larger sequence.\n\n## The Building Blocks\n\nWe'll use two main classes to explore this idea:\n- `Rule`: Represents a production rule in our grammar, like \"S -\u003e A B\" meaning \"S can be replaced with A followed by B\"\n- `Grammar`: A collection of rules that can generate patterns, with methods to calculate how many bits we need to store it\n\nYou can find the executable code in [`grammar.py`](https://github.com/sibeshkar/sibeshkar.github.io/blob/hugo/content/code/grammar.py).\n\n## Approach 1: Naive Grammar - Simple Memorization\n\nLet's start with the most straightforward approach - just writing down what we see:\n\n```python\n# Create a rule for each Fibonacci number\nnaive_rules = [Rule('S', ['F1'])] + [\n    Rule(f'F{i+1}', [str(num)])\n    for i, num in enumerate(fib_sequence)\n]\nnaive_grammar = Grammar(naive_rules)\nnaive_bits = naive_grammar.description_length()\nprint(f\"Naive grammar needs {naive_bits:,} bits\")\nprint(f\"Compression ratio: {direct_bits/naive_bits:.2f}x\")\n```\n```\nNaive grammar needs 892 bits\nCompression ratio: 2.97x\n```\n\nThis represents the most basic level of pattern recognition:\n- ‚úì Noticing that numbers can be labeled (F1, F2, etc.)\n- ‚úì Understanding sequence order\n- ‚úó No understanding of relationships between numbers\n- ‚úó No recognition of the underlying pattern\n\nIt's like a student who memorizes \"1, 1, 2, 3, 5, 8...\" without understanding why these numbers appear in this order. They achieve some compression just by being organized, but they're still essentially memorizing. Suppose we have 5.6TB of text data, ethically scraped from the internet, broken into word pairs (2-grams) stored in a lookup table. When asked to complete \"I was going to wear a...\", it might meaninglessly output \"a lot\" because \"a lot\" appears more frequently than \"a shirt\" or \"a skirt\". A 3-gram model, using two words of context, improves accuracy but still fails in cases like \"It's raining outside, wear a...\". A memorized table like this would be an example of a generative models - predictingthe statistically-most-likely next word based on patterns in the data.\n\n## Approach 2: Pattern Recognition - Seeing Relationships\n\nNow we start to notice relationships between numbers. This requires more sophisticated observation:\n\n```python\npattern_rules = [\n    Rule('S', ['F', 'N1']),\n    Rule('N1', ['1']),\n] + [\n    Rule(f'N{fib}', [str(fib)])\n    for fib in sorted(set(fib_sequence[:20]))  # First 20 unique numbers\n] + [\n    Rule('F', ['N1']),\n    Rule('F', ['N1', '+', 'N1']),  # 1 + 1 = 2\n    Rule('F', ['N2', '+', 'N3']),  # 2 + 3 = 5\n    Rule('F', ['N3', '+', 'N5']),  # 3 + 5 = 8\n    Rule('F', ['N5', '+', 'N8']),  # 5 + 8 = 13\n]\npattern_grammar = Grammar(pattern_rules)\npattern_bits = pattern_grammar.description_length()\nprint(f\"Pattern grammar needs {pattern_bits:,} bits\")\nprint(f\"Compression ratio: {direct_bits/pattern_bits:.2f}x\")\n```\n```\nPattern grammar needs 428 bits\nCompression ratio: 6.19x\n```\n\nThis represents an intermediate level of understanding:\n- ‚úì Recognition that numbers are related through addition\n- ‚úì Ability to see specific instances of the pattern\n- ‚úó Still manually writing out each addition step\n- ‚úó No recognition of the recursive nature\n- ‚úó Can't generate numbers beyond what's explicitly coded\n\nIt's like a student who realizes \"Oh, I can get each number by adding specific previous numbers!\" They're starting to see relationships, but they're still writing out each step manually. They might even make a table:\n```\n1 + 1 = 2\n2 + 3 = 5\n3 + 5 = 8\n```\nThis is better than memorization, but they haven't yet had their \"aha!\" moment.\n\n## Approach 3: Full Abstraction - The Cognitive Leap\n\nFinally, we make the cognitive leap to understand the deep structure:\n\n```python\nabstract_rules = [\n    Rule('S', ['F', '1']),\n    Rule('F', ['1']),\n    Rule('F', ['F', '+', 'F_prev'])\n]\nabstract_grammar = Grammar(abstract_rules)\nabstract_bits = abstract_grammar.description_length()\nprint(f\"Abstract grammar needs {abstract_bits:,} bits\")\nprint(f\"Compression ratio: {direct_bits/abstract_bits:.2f}x\")\n```\n```\nAbstract grammar needs 14 bits\nCompression ratio: 189.21x\n```\n\nThis represents the highest level of understanding, requiring several cognitive breakthroughs:\n- ‚úì Recognition that each number depends on the previous TWO numbers\n- ‚úì Understanding that this single relationship explains EVERY number\n- ‚úì Grasping that you don't need to store the numbers themselves\n- ‚úì Realizing the pattern is recursive and self-contained\n- ‚úì Understanding that this works for ANY length sequence\n\nThe cognitive steps to reach this understanding typically involve:\n1. Noticing that you're always adding two numbers\n2. Realizing those two numbers are always the previous ones\n3. The key insight: this ONE rule explains EVERYTHING\n4. Understanding that with just this rule and a starting point, you can generate the entire sequence\n\nIt's like the student who suddenly exclaims \"Wait... we're ALWAYS just adding the last two numbers! That's all we need to know!\" This is the moment of true understanding, where the pattern becomes crystal clear and beautifully simple.\n\n## Generating the Sequence\n\nThe beauty of this abstract grammar is that it's not just for compression - we can use it to regenerate the original sequence or extend it to any length we want:\n\n```python\ndef generate_fibonacci(n: int) -\u003e List[int]:\n    \"\"\"Generate first n Fibonacci numbers using our grammar rules.\"\"\"\n    sequence = [1, 1]  # Initial state from our grammar's rules\n    for _ in range(n-2):\n        sequence.append(sequence[-1] + sequence[-2])  # F -\u003e F + F_prev rule\n    return sequence\n\n# Test the generator\nprint(\"Regenerating our sequence:\")\nprint(generate_fibonacci(10))  # First 10 numbers\nprint(\"\\nExtending beyond what we originally stored:\")\nprint(generate_fibonacci(15))  # First 15 numbers\n```\n```\nRegenerating our sequence:\n[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n\nExtending beyond what we originally stored:\n[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]\n```\n\nThis demonstrates the true power of understanding the generative process: with just 14 bits of grammar rules, we can:\n1. Reproduce the original sequence exactly\n2. Generate any Fibonacci number we want\n3. Extend the sequence infinitely\n\nThe massive improvement in compression ratio (189.21x vs 2.97x) reflects this deep understanding. We've gone from:\n- Memorizing 100 numbers (naive) ‚Üí\n- Understanding specific additions (pattern) ‚Üí\n- Grasping the universal rule (abstract)\n\nThis progression mirrors how humans learn: from memorization, to pattern recognition, to true understanding. The fact that better understanding leads to better compression isn't just a mathematical curiosity - it's a fundamental principle of how we make sense of the world.\n\nLet's visualize these compression ratios:\n\n```python\nprint(\"\\nCompression Comparison:\")\nprint(\"=\" * 50)\nprint(f\"Direct storage:     {direct_bits:6,} bits (baseline)\")\nprint(f\"Naive grammar:      {naive_bits:6,} bits ({direct_bits/naive_bits:6.2f}x better)\")\nprint(f\"Pattern grammar:    {pattern_bits:6,} bits ({direct_bits/pattern_bits:6.2f}x better)\")\nprint(f\"Abstract grammar:   {abstract_bits:6,} bits ({direct_bits/abstract_bits:6.2f}x better)\")\n```\n```\nCompression Comparison:\n==================================================\nDirect storage:      2,649 bits (baseline)\nNaive grammar:         892 bits (  2.97x better)\nPattern grammar:       428 bits (  6.19x better)\nAbstract grammar:       14 bits (189.21x better)\n```\n\nThis dramatic improvement shows the true power of finding the underlying generative process. The more data we have, the more valuable it becomes to understand the true pattern rather than just storing or partially compressing the output.\n\n## From Numbers to Language\n\nLet's apply the same thinking to language patterns. Here's a set of similar sentences:\n\n```python\nsentences = [\n    \"I like cats and dogs\",\n    \"I like books and music\",\n    \"I like coffee and tea\",\n    \"I like movies and games\"\n]\ndirect_bits = sum(len(s) * 8 for s in sentences)  # 8 bits per char\nprint(f\"Direct storage needs {direct_bits} bits\")\n```\n```\nDirect storage needs 688 bits\n```\n\nThat's a lot of bits! Let's try our different approaches:\n\n### Naive Grammar (Store Each Sentence):\n```python\nnaive_rules = [\n    Rule('S', ['I', 'like', 'cats', 'and', 'dogs']),\n    Rule('S', ['I', 'like', 'books', 'and', 'music']),\n    Rule('S', ['I', 'like', 'coffee', 'and', 'tea']),\n    Rule('S', ['I', 'like', 'movies', 'and', 'games'])\n]\nnaive_grammar = Grammar(naive_rules)\nnaive_bits = naive_grammar.description_length()\nprint(f\"Naive grammar needs {naive_bits} bits (compression ratio: {direct_bits/naive_bits:.2f}x)\")\n```\n```\nNaive grammar needs 63 bits (compression ratio: 10.92x)\n```\n\nJust by recognizing words as reusable symbols, we get almost 11x compression!\n\n### Pattern Recognition (Find Common Structure):\n```python\npattern_rules = [\n    Rule('S', ['I', 'like', 'THING', 'and', 'THING']),\n    Rule('THING', ['cats']), Rule('THING', ['dogs']),\n    Rule('THING', ['books']), Rule('THING', ['music']),\n    Rule('THING', ['coffee']), Rule('THING', ['tea']),\n    Rule('THING', ['movies']), Rule('THING', ['games'])\n]\npattern_grammar = Grammar(pattern_rules)\npattern_bits = pattern_grammar.description_length()\nprint(f\"Pattern grammar needs {pattern_bits} bits (compression ratio: {direct_bits/pattern_bits:.2f}x)\")\n```\n```\nPattern grammar needs 30 bits (compression ratio: 22.93x)\n```\n\nBy recognizing that we can reuse the pattern \"I like X and Y\", we get even better compression!\n\n### Full Abstraction (Separate First and Second Items):\n```python\nabstract_rules = [\n    Rule('S', ['I', 'like', 'N1', 'and', 'N2']),\n    Rule('N1', ['cats', 'books', 'coffee', 'movies']),\n    Rule('N2', ['dogs', 'music', 'tea', 'games'])\n]\nabstract_grammar = Grammar(abstract_rules)\nabstract_bits = abstract_grammar.description_length()\nprint(f\"Abstract grammar needs {abstract_bits} bits (compression ratio: {direct_bits/abstract_bits:.2f}x)\")\n```\n```\nAbstract grammar needs 39 bits (compression ratio: 17.64x)\n```\n\nInterestingly, trying to be too clever (separating first and second items) actually hurts our compression! This is a key insight: the best compression comes from matching the true structure of the data.\n\n### The Challenge of English Grammar\n\nHowever, English presents a challenge for such clean abstractions. Unlike our previous examples, English is a context-sensitive language, which means the interpretation of a word or phrase often depends on its context. For example:\n\n```python\ncontext_examples = [\n    \"The bank is closed\" ,           # Financial institution\n    \"The bank is muddy\",            # River bank\n    \"I bank on your support\",       # Rely/depend\n    \"The plane will bank left\"      # Aviation term\n]\n```\n\nThis context sensitivity means the same word can have different meanings based on surrounding words, the same grammatical structure can have different interpretations, and valid combinations depend on semantic meaning, not just syntax.  \n\nThis limits how much we can compress English using pure grammatical rules. Our compression ratio of 22.93x was possible because we used a very restricted subset of English. In the general case, we would need better rules about how context influences meaning, better word sense disambiguation etc.\n\nThis is why formal languages (like programming languages) and some natural languages with more rigid structure achieve better compression ratios.\n\n### A More Compressible Language\n\nsome more ancient languages like Sanskrit (and Greek) turn out to have a structure more amenable to rule-based generation. This isbecause:\n1. Words are derived from root forms using explicit rules\n2. Compound words follow clear compositional rules\n3. Sentence structure has more rigid constraints\n4. Word meanings are more systematically related to their roots\n\nThis makes a language like Sanskrit more like our Fibonacci sequence - there are clear generative rules that can produce valid constructions.\n\n### PƒÅ·πáini's Method in Action\n\nLet's look at some concrete examples of how PƒÅ·πáini's grammar generates Sanskrit words and sentences:\n\n1. **Root-Based Word Generation**:\n   ```python\n   # Example: Generating forms of \"bh≈´\" (to be)\n   root_rules = [\n       Rule('VERB', ['ROOT', 'SUFFIX']),\n       Rule('ROOT', ['bh≈´']),  # \"to be\"\n       Rule('SUFFIX', ['ami']),  # 1st person present\n       Rule('SUFFIX', ['asi']),  # 2nd person present\n       Rule('SUFFIX', ['ati'])   # 3rd person present\n   ]\n   ```\n   This generates:\n   - bhavƒÅmi (I am)\n   - bhavasi (you are)\n   - bhavati (he/she/it is)\n\n   The transformation bh≈´ ‚Üí bhav is handled by another rule (gu·πáa strengthening). When the root 'bh≈´' combines with certain suffixes, this rule changes the '≈´' sound to 'av', demonstrating how PƒÅ·πáini's system handles systematic sound (phonological) changes that occur when morphemes combine.\n\n2. **Compound Word Formation**:\n   ```python\n   # Example: Generating compound words\n   compound_rules = [\n       Rule('COMPOUND', ['WORD1', 'WORD2']),\n       Rule('WORD1', ['rƒÅja']),  # king\n       Rule('WORD2', ['putra'])  # son\n   ]\n   sandhi_rules = [\n       Rule('SANDHI', ['a', 'a'], ['ƒÅ']),  # a + a ‚Üí ƒÅ\n   ]\n   ```\n   This generates:\n   - rƒÅja + putra ‚Üí rƒÅjaputra (king's son)\n   The rules handle both combination and sound changes (sandhi).\n\n3. **Sentence Structure**:\n   ```python\n   # Example: Generating active/passive sentences\n   sentence_rules = [\n       Rule('S', ['NP', 'VP']),\n       Rule('NP', ['devadatta']),  # Devadatta (name)\n       Rule('VP', ['odana·πÉ', 'pacati']),  # rice + cooks\n       Rule('VP_PASSIVE', ['odana·∏•', 'pacyate'])  # rice + is cooked\n   ]\n   ```\n   This generates:\n   - devadatta·∏• odana·πÉ pacati (Devadatta cooks rice)\n   - odana·∏• pacyate (Rice is cooked)\n\nThe power of PƒÅ·πáini's system comes from how these rules interact:\n\n1. **Recursive Application**:\n   ```python\n   # Example: Complex word formation\n   derivation_rules = [\n       Rule('WORD', ['ROOT', 'PRIMARY_SUFFIX', 'SECONDARY_SUFFIX']),\n       Rule('ROOT', ['bh≈´']),\n       Rule('PRIMARY_SUFFIX', ['ana']),  # action noun\n       Rule('SECONDARY_SUFFIX', ['tva'])  # abstract quality\n   ]\n   ```\n   This generates:\n   - bh≈´ ‚Üí bhavana (becoming) ‚Üí bhavanatva (the quality of becoming)\n\n2. **Meta-Rules**:\n   ```python\n   # Example: Rule ordering\n   meta_rules = [\n       Rule('APPLY_ORDER', ['ROOT_RULES', 'SANDHI_RULES', 'ACCENT_RULES']),\n       Rule('EXCEPTION', ['if_final_position', 'skip_sandhi'])\n   ]\n   ```\nThese meta-rules ensure correct application order and handle exceptions systematically.\n\nWe see PƒÅ·πáini's approach compresses his original corpus by many orders of magnitude by systematically doing *more* with *less*:\n1. Each rule can generate many forms (e.g., one verb root rule generates hundreds of conjugations)\n2. Rules interact to produce complex forms (like compounds and derivatives)\n3. Meta-rules handle exceptions without needing separate rules for each case\n4. The system captures both form (phonology) and meaning (semantics)\n\nFor example, from just the root \"bh≈´\" and a set of rules, PƒÅ·πáini's grammar can generate:\n- All conjugated forms (bhavƒÅmi, bhavasi, bhavati, etc.)\n- All derived nouns (bhavana, bhƒÅva, bh≈´ti, etc.)\n- All compounds (bh≈´loka - world of existence, bh≈´tap≈´rva - having been before, etc.)\n- All these forms in different syntactic roles\n\nThis is analogous to our Fibonacci example, where one simple rule (Fn = Fn-1 + Fn-2) generates an infinite sequence. But PƒÅ·πáini's grammar goes further, handling multiple interacting patterns simultaneously while maintaining semantic coherence. There are infact two remarkable things to notice here: \n\n1. He was building on some previous work, but that a human with no tooling/computation access could speedrun a 5000x compression in a lifetime is astonishing. A 21st century analogue would be if Linus Torvalds hand-wrote the final, complete, version of Linux on his first try without access to a compiler - the final version which would never require any patches/fixes after creation. \n2. It's bizarre that an unchanging centralized rule-based generative order even exists for an evolving spoken language that was decentralized in its usage - and this formalism was lying there waiting to be discovered, like one would think of a theory of gravity. It's like counting the seed spirals of a sunflower and finding that they always follow some predetermined recursive Fibonacci sequence ([which they do!](https://royalsocietypublishing.org/doi/10.1098/rsos.160091)), or appear in fixed golden ratios of Phi (1.618...). This is a discussion for another day.\n\n\u003e \"The descriptive grammar of Sanskrit which PƒÅ·πáini brought to it's highest perfection is one of the greatest monuments of human intelligence\" - L. Bloomfield, father of American distributionalism\n\nHow is PƒÅ·πáini's ~4000 rule set a compressed generative model like one would understand an LLM to be? Imagine prompting an LLM like ChatGPT with an empty prompt, and letting it run for a few paragraphs. If you did this a trillion times, with slightly different temperature settings, you would eventually recover a slightly morphed version of the entire corpus it was trained on. 'Model distillation' is the industry term for this common practice, used to copy parts of another LLM's training set. In a similar manner, if you were to run the state machine described in the *A·π£·π≠ƒÅdhyƒÅyƒ´* a billion times with random inputs, you would eventually recover all the Vedic hymns, mantras, and Brahmanas it compresses, include the different variations of spoken Sanskrit. Of course, just like you guide an LLM's output by including an input prompt, you would need to guide the state machine in the *A·π£·π≠ƒÅdhyƒÅyƒ´* to generate the *kind* of sentences you wanted. But the complexity of the original source remains contained in the abstracted, highly compressed rule set.\n\nThis brings us to a crucial insight about building machine intelligence. The key to achieving a ~5000:1 compression ratio like PƒÅ·πáini did, lies not in the lookup tables of Searle's Chinese Room, but in the process that created those tables - the hidden PƒÅ·πáini-like compressor that could derive rules like the one above through the repeated application of abstraction and economy. Let's next try to understand what making an automated grammarian might look like.\n\n## From Numbers to PƒÅ·πáini: The First Computational Grammarian\n\nHow might PƒÅ·πáini have done it? let's look at our own process of compressing the Fibonacci sequence:\n\n1. Start with examples (like our Fibonacci numbers or sentences)\n2. Look for patterns (like our pattern recognition phase)\n3. Abstract to rules (like our final recursive sgrammars)\n\nPƒÅ·πáini likely followed a similar path:\n1. **Data Collection**: Gathered/recorded thousands of source material (~10,000 hours of spoken Sanskrit)\n2. **Pattern Recognition**: Identified recurring structures\n3. **Rule Abstraction**: Derived minimal generative rules, meta-rules, and exceptions\n4. **Optimization**: Compressed rules for memorization (~reduced to 2 hours of ~4000 rules that can be memorized)\n\nIn the case of the fibonacci sequence, the method achieves a compression ratio of 189.21x, in the case of Sanskrit, PƒÅ·πáini's grammar achieves a compression ratio of ~5000:1:.\n\nWhat makes PƒÅ·πáini's work particularly relevant to our discussion is that it demonstrates the same principles we've discovered with compressing Fibonacci sequences:\n\n1. **Cognitive Progression**: Like our Fibonacci example progressing from naive to abstract, from memorizing words to understanding derivation rules\n\n2. **Minimal Description**: Like our abstract Fibonacci grammar using just 3 rules, his grammar captures an entire language in ~4,000 rules\n\n3. **Generative Power**: Like our abstract grammars generating infinite sequences, his system can generate all valid Sanskrit constructions\n\n## From Ancient Grammar to Modern Systems : The Case of Pong\n\nWhile PƒÅ·πáini's work on Sanskrit grammar might seem purely academic, this method of identifying patterns, finding minimal rules, and using them to generate valid outputs can be applied far beyond language - it provides a universal framework for understanding and compressing any complex system, allowing us to generate infinite valid states from a small set of core rules.\n\nThe real power of PƒÅ·πáini's compression approach to generative grammars becomes clear when we apply it to modelling more complex systems with real-world state machines - like video games. Just as PƒÅ·πáini found that all of Sanskrit could be generated from ~4,000 rules, we'll see how an entire game can be generated from just 8 rules. Let's look at how we could compress a recording of a Pong game:\n\n```python\nclass PongGrammar:\n    def __init__(self):\n        self.rules = [\n            Rule('GAME', ['INIT', 'LOOP']),\n            Rule('INIT', ['Ball(center)', 'Paddle1(mid)', 'Paddle2(mid)', 'Score(0,0)']),\n            Rule('LOOP', ['UPDATE', 'COLLISIONS', 'SCORE', 'LOOP']),\n            Rule('UPDATE', ['Ball(pos += vel)', 'Paddle1(pos += input1)', 'Paddle2(pos += input2)']),\n            Rule('COLLISIONS', ['WALL_CHECK', 'PADDLE_CHECK']),\n            Rule('WALL_CHECK', ['if ball.y \u003e height: ball.vel.y *= -1']),\n            Rule('PADDLE_CHECK', ['if ball.collides(paddle): ball.vel.x *= -1']),\n            Rule('SCORE', ['if ball.x \u003c 0: p2_score++ else if ball.x \u003e width: p1_score++']),\n        ]\n        self.grammar = Grammar(self.rules)\n\n# Let's calculate sizes for a 5-minute game at 60 FPS\nframes = 300 * 60  # 5 minutes at 60 FPS\n\n# First, let's look at a conservative estimate (just storing positions)\npos_frame_size = (4 + 4 + 8)  # 4 bytes each for paddles, 8 for ball position\npos_raw_size = pos_frame_size * frames\ngrammar_size = pong.grammar.description_length()  # Rules of the game\ninput_size = 2 * frames  # 1 bit per paddle per frame\nstate_size = 16  # 4 bytes each for ball x,y and two paddle positions\n\nprint(\"Conservative Estimate (Position Data Only)\")\nprint(\"=========================================\")\nprint(f\"Raw position data:   {pos_raw_size:,} bytes ({pos_raw_size/1024:.1f} KB)\")\nprint(f\"Grammar + state:     {grammar_size + input_size + state_size:,} bytes ({(grammar_size + input_size + state_size)/1024:.1f} KB)\")\nprint(f\"Compression ratio:   {pos_raw_size/(grammar_size + input_size + state_size):.2f}x\")\n\n# Now let's look at the full visual data\nframe_height = 210\nframe_width = 160\nchannels = 3  # RGB\npixel_frame_size = frame_height * frame_width * channels  # bytes per frame\npixel_raw_size = pixel_frame_size * frames  # Full video storage\n\nprint(\"\\nFull Visual Data Estimate (Every Pixel)\")\nprint(\"=======================================\")\nprint(f\"Frame dimensions: {frame_width}x{frame_height} pixels (RGB)\")\nprint(f\"Storage requirements:\")\nprint(f\"- Raw video:        {pixel_raw_size:,} bytes ({pixel_raw_size/1024/1024:.1f} MB)\")\nprint(f\"- Grammar rules:    {grammar_size:,} bytes\")\nprint(f\"- Player inputs:    {input_size:,} bytes\")\nprint(f\"- Game state:       {state_size:,} bytes\")\nprint(f\"- Total compressed: {grammar_size + input_size + state_size:,} bytes ({(grammar_size + input_size + state_size)/1024:.1f} KB)\")\nprint(f\"Compression ratio:  {pixel_raw_size/(grammar_size + input_size + state_size):,.2f}x\")\n\n# To put this in perspective:\nprint(\"\\nTo store a 5-minute Pong game:\")\nprint(\"1. Conservative Approach (Just Positions)\")\nprint(f\"   - Raw data: {pos_raw_size/1024:.1f} KB\")\nprint(f\"   - Compressed: {(grammar_size + input_size + state_size)/1024:.1f} KB\")\nprint(f\"   - Ratio: {pos_raw_size/(grammar_size + input_size + state_size):.2f}x better\")\nprint(\"\\n2. Full Visual Approach (Every Pixel)\")\nprint(f\"   - Raw data: {pixel_raw_size/1024/1024:.1f} MB\")\nprint(f\"   - Compressed: {(grammar_size + input_size + state_size)/1024:.1f} KB\")\nprint(f\"   - Ratio: {pixel_raw_size/(grammar_size + input_size + state_size):,.2f}x better\")\n```\n```\nConservative Estimate (Position Data Only)\n=========================================\nRaw position data:   1,872,000 bytes (1,828.1 KB)\nGrammar + state:     38,064 bytes (37.2 KB)\nCompression ratio:   49.18x\n\nFull Visual Data Estimate (Every Pixel)\n=======================================\nFrame dimensions: 160x210 pixels (RGB)\nStorage requirements:\n- Raw video:        18,144,000,000 bytes (17,304.7 MB)\n- Grammar rules:    2,048 bytes\n- Player inputs:    36,000 bytes\n- Game state:       16 bytes\n- Total compressed: 38,064 bytes (37.2 KB)\nCompression ratio:  476,671.45x\n\nTo store a 5-minute Pong game:\n1. Conservative Approach (Just Positions)\n   - Raw data: 1,828.1 KB\n   - Compressed: 37.2 KB\n   - Ratio: 49.18x better\n\n2. Full Visual Approach (Every Pixel)\"\n   - Raw data: 17,304.7 MB\n   - Compressed: 37.2 KB\n   - Ratio: 476,671.45x better\n```\n\nThis is remarkable in two ways:\n\n1. **Conservative Estimate** (Just storing positions):\n   - Raw data: ~1.8 MB\n   - Compressed: ~37 KB\n   - Nearly 50x compression just for the game state!\n\n2. **Full Visual Data** (Every pixel of every frame):\n   - Raw data: ~17.3 GB\n   - Compressed: ~37 KB\n   - Almost 500,000x compression!\n\nEven our conservative estimate shows impressive compression because we're capturing the underlying game logic. But when we consider that this same grammar can generate the complete visual output, the compression becomes staggering. We achieve this because:\n\n1. The rules of the game (our grammar) - about 2KB\n2. The player inputs - about 35KB\n3. The game state - just 16 bytes\n\nThe grammar captures both the physics engine and the rendering logic in just 8 rules! This massive compression ratio illustrates a profound point: when we truly understand a system, we don't need to store its behavior - we can store its rules and regenerate any behavior we want. This is exactly how human intelligence works: we don't memorize every position or pixel of every object we've ever seen moving - we learn the intuitive laws of physics and can use them to predict and understand any motion.\n\n## The Connection to Intelligence\n\nThe relationship between compression and intelligence becomes clear when we look at our progression from simple sequences to complex systems. In each case, better compression came from better understanding (\"modelling\") of the dynamics of the system:\n\n1. For Fibonacci, understanding the recursive relationship led to 189.21x compression\n2. For the English sentences, recognizing sentence structure led to 22.93x compression (limited by English's context sensitivity)\n3. For Sanskrit, PƒÅ·πáini's grammar achieved remarkable compression of an entire language with just ~4,000 rules (~5000:1 compression ratio)\n4. For Pong, applying PƒÅ·πáini's principles to game physics led to 476,671.45x compression\n\nThis progression - from numbers to language to video games - shows how the same fundamental principles of finding minimal generative descriptions apply across domains. This is why compression can be seen as a measure of intelligence: the better we understand a system, the more efficiently we can describe it.\n\nIn machine learning terms, this is closely related to the concept of \"minimum description length\" (MDL) principle, which states that the best model for a dataset is the one that minimizes the size of the model (our grammar rules) and maximizes the size of the data it explains (our inputs)\n\nThis principle is formalized by Ray Solomonoff in his theory of universal inductive inference. \n\n\u003e \"If the universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset\"\n\nThe connection to Solomonoff induction helps explain why PƒÅ·πáini's grammar has remained useful for over two millennia: by finding the shortest possible rule set that could generate Sanskrit, he wasn't just being clever with compression - he was discovering the true underlying structure of the language. Finding the most compressed representation of data (like PƒÅ·πáini's grammar rules) isn't just an efficiency trick - it's mathematically optimal for prediction and understanding.\n\nThis connection between compression and understanding has profound implications:\n\n**Learning** is finding better, shorter abstract grammars for observed data.\n\n**Intelligence** can be measured by ability to find compact descriptions in the fewest observations (e.g. by looking at the least number of Fibonacci numbers).\n\n**Understanding** means finding the true generative process that produces past and future observations.\n\nWhen a child learns how to catch a ball, they're not memorizing every position of the ball they've seen - they're *discovering* the grammar of projectile motion, that they can apply to any object hurled towards them. When we understand language, we don't memorize every possible sentence - we learn the rules that generate valid ones. When we play chess, we don't memorize every possible board position - we discover the rules of the game, and even discover new high-level rules that the original game designer hadn't thought of - like the *Queen's Gambit* opening move. When PƒÅ·πáini created his grammar, he wasn't just documenting Sanskrit - he was discovering a fundamental approach to understanding that we can apply to everything from ancient languages to modern video games.\n\nIndeed, this is what we see with our code examples above and with what PƒÅ·πáini's astonishing feat of compression. His work isn't just limited to modelling the spoken linguistic reality of the past - but could well be about discovering a universal principle of intelligence that will hold the key to building powerful thinking machines of the future.\n\n\u003e \"Riemann invented his geometries before Einstein had a use for them; the physics of our universe is not that complicated in an absolute sense.  A Bayesian superintelligence, hooked up to a webcam, would invent General Relativity as a hypothesis‚Äîperhaps not the dominant hypothesis, compared to Newtonian mechanics, but still a hypothesis under direct consideration‚Äîby the time it had seen the third frame of a falling apple.  It might guess it from the first frame, if it saw the statics of a bent blade of grass.\" - E. Yudkowsky\n\n## References\n\n[1] [Youtube video](https://www.youtube.com/watch?v=dO4TPJkeaaU), Compression for AGI, Jack Rae, Stanford MLSys, ex-OpenAI, 2023\n\n[2] [Youtube Playlist](https://www.youtube.com/playlist?list=PLsAPTmdVuspykLNnjs1_zQKRMqRRfDr2R), PƒÅ·πáini Lecture Series, Dr. Saroja Bhate, Bangalore International Center, 2023\n\n[3] The *A·π£·π≠ƒÅdhyƒÅyƒ´* achieves a remarkable compression ratio of at least 5000:1, condensing the rules that can generate over 10,000 hours of attested Sanskrit literature (including the ~20,000 verses of the four Vedas, along with the 100,000 verses of the Mahabharata, 24,000 verses of the Ramayana, 400,000 verses of the Puranas, and hundreds of thousands of verses across texts, philosophical shastras, and classical poetry) into just 2 hours of precisely formulated rules.\n\n[4] PƒÅ·πáini: Catching the Ocean in a Cow's Hoofprint, Vikram Chandra, 2019[blog.granthika.co/panini/](https://blog.granthika.co/panini/). Brilliant reading on the topic. [Excerpt](https://x.com/sibeshkar/status/1889311803907227839)","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/panini_mid_old":{"title":"The PƒÅ·πáinian Approach to Compression","content":"\u003e \"The simplest of several competing explanations is likely to be the correct one\" - Occam's Razor\n\n## Intelligence as compression\n\nLet's examine a modified version of Searle's Chinese room experiment. Suppose we have 5.6TB of text data, ethically scraped from the internet, broken into word pairs (2-grams) stored in a lookup table. When asked to complete \"I was going to wear a...\", it might meaninglessly output \"a lot\" because \"a lot\" appears more frequently than \"a shirt\" or \"a skirt\". A 3-gram model, using two words of context, improves accuracy but still fails in cases like \"It's raining outside, wear a...\". Both are examples of generative models: they predict the statistically-most-likely next word based on patterns in the data.\n\nLarge Language Models (LLMs) like DeepSeek, GPT, or Claude are more sophisticated versions of such generative models. They use thousands of tokens of context and the 'attention' mechanism to focus on relevant parts of the input to 'query' with.  They're not just storing a memorized table of what comes after what, but using compute to extract and store a hiearchy of reusable chunks of information in it's layers. A result is that the weights of the Llama-65B model occupy around 365GB on disk, down from the 5.6TB it's trained on (a 14x compression)[1]. We can see that generalization ability and data efficiency are equivalent: generalization comes from squeezing every bit of information out of your datapoints, understanding all correlations and causations, and connecting all the dots. \"Squeezing every bit of information\" is meant literally: generalization is the very direct result of compression. \n\nYet, LLMs are still many orders-of-magnitude less data-efficient than humans. Lee Sedol, a top Go player, played around 10,000 games in his lifetime, while DeepMind's bot AlphaGo required 30 million games to match him. If Sedol had played 30 million games, how skilled would he be? What would a human who has absorbed all of human knowledge look like? How does the human generative model compress information so effectively? The answer to these question is the key to building machines that think, learn, adapt to tasks like (or better than) humans do, i.e. general machine intelligence.\n\nBut it's hard to run this experiment because most humans have seen orders of magnitude less data in their lifetimes, and it's hard to manually inspect human priors. Or so I thought, until I attended the lecture series[2] by Dr. Saroja Bhate at Bangalore International Centre, on PƒÅ·πáini (pronounced \"pah-nee-nee\"), the ancient Sanskrit grammarian. Over 2300 years ago, before the advent of computers or formal logic, PƒÅ·πáini sat down and methodically reduced all of human knowledge, then floating around in spoken Vedic Sanskrit, into a generative grammar of exactly 3,995 *s≈´tras*, or rewrite rules - recorded in his magnum opus, the *A·π£·π≠ƒÅdhyƒÅyƒ´*. These rules have remained unchanged ever since.\n\nThere is a small pool of postdoctoral Sanskrit scholars who are fortunate to understand the full magnitude of PƒÅ·πáini's achievement, but I will try to do some justice to it. Faced with a large corpus of spoken Vedic and contemporary Sanskrit, many thousands of hours of audio signals collected without any substrate to record with or automated tooling to work with, PƒÅ·πáini, over 12 years, found abstracted atoms of meaning that when combined with a set of dynamic rules and meta-rules formed a generative grammar, a deterministic state machine that could be used to re-synthesize the original audio corpus - and be recited in just 2 hours (an astonishing compression ratio of atleast ~5000:1)[3].  \n\nHow is PƒÅ·πáini's ~4000 rule set a compressed generative model like one would understand an LLM to be? Imagine prompting an LLM like ChatGPT with an empty prompt, and letting it run for a few paragraphs. If you did this a trillion times, with slightly different temperature settings, you would eventually recover a slightly morphed version of the entire corpus it was trained on. 'Model distillation' is the industry term for this common practice, used to copy parts of another LLM's training set. In a similar manner, if you were to run the state machine described in the *A·π£·π≠ƒÅdhyƒÅyƒ´* a billion times with random inputs, you would eventually recover all the Vedic hymns, mantras, and Brahmanas it compresses, include the different variations of spoken Sanskrit. Of course, just like you guide an LLM's output by including an input prompt, you would need to guide the state machine in the *A·π£·π≠ƒÅdhyƒÅyƒ´* to generate the *kind* of sentences you wanted. But the complexity of the original source remains contained in the abstracted, highly compressed rule set.\n\n\u003e \"If the universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset\" - Ray Solomonoff, inventor of algorithmic probability\n\nPƒÅ·πáini gives the formation of every inflected, compounded, or derived word, with an exact statement of the sound-variations (including accent) and of the meaning. His foresight in designing these rules means they have stood the test of time - not only expressive enough to explain the knowledge of the past at the time, but also to generalize to phrases and sentences in the future since then. His work is the first formal system known to man, doing to linguistic reality what Euclid would go on to do later for geometry, but it would be no overstatement to call it the most impressive human act of knowledge compression till date. It is the only example we have of true optimal [Solomonoff induction](https://en.m.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference), finding the shortest executable archive of a dataset, as evidenced by it's durability over time.\n\nOf course, PƒÅ·πáini was working with Sanskrit, whose underlying structure makes it less context-sensitive than English and more amenable to such decomposition. But for the sake of comparison, his methods if automated and applied to the [Hutter compression prize](http://prize.hutter1.net/) dataset could compress 1GB of Wikipedia data to a few kilobytes (down from the current record of 110MB as of Feb 2025). A digital superintelligence in action, would very much employ what could be called \"PƒÅ·πáini's Razor\" - applying the dual techniques of abstraction and economy ruthlessly to thousands of noisy signals of various forms and fidelity, reducing them by many orders of magnitude into succinct set of formal predicates - deriving an explanation unchanging in time. Like breaking down a house into basic Lego-like blocks and then building a new house from it back gain, a machine like this could then combine these discovered reusable concepts on the fly using abstracted transformation rules to generalize to any unknown. \n\n\u003e \"The descriptive grammar of Sanskrit which PƒÅ·πáini brought to it's highest perfection is one of the greatest monuments of human intelligence\" - L. Bloomfield, father of American distributionalism\n\nIt's worth examining the implications of this. To build powerful thinking machines of the future that can compress information and generalize better than humans do, we must dig up and study in great detail the forgotten relic that is PƒÅ·πáini's Razor. \nLet's examine this idea in detail through concrete examples, starting with something simple and building up to more complex systems.\n\n# Understanding Generative Grammars Through Compression\n\nDisclaimer : I have no formal training in Sanskrit, and the following is merely a programmer's attempt to reverse-engineer PƒÅ·πáini's methods of compression. This is largely a guide for programmers, so we dive into code right away. Let's first look at how generative grammars compress information. All mistakes are mine.\n\nThis is largely a guide for programmers, so we dive into code right away. Let's first look at how generative grammars compress information.\n\nImagine you're trying to send the first 100 Fibonacci numbers to a friend. The naive approach would be to simply transmit these numbers directly. Let's see just how big this sequence gets:\n\n```python\ndef generate_fibonacci(n: int) -\u003e List[int]:\n    \"\"\"Generate first n Fibonacci numbers.\"\"\"\n    sequence = [1, 1]\n    for _ in range(n-2):\n        sequence.append(sequence[-1] + sequence[-2])\n    return sequence\n\nfib_sequence = generate_fibonacci(100)\nprint(\"First 100 Fibonacci numbers:\")\nprint(\", \".join(str(x) for x in fib_sequence[:10]) + \"...\")\nprint(f\"100th Fibonacci number: {fib_sequence[-1]:,}\")\ndirect_bits = sum(math.ceil(math.log2(x)) for x in fib_sequence)\nprint(f\"\\nDirect storage needs {direct_bits:,} bits\")\n```\n```\nFirst 100 Fibonacci numbers:\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\n100th Fibonacci number: 354,224,848,179,261,915,075\n\nDirect storage needs 2,649 bits\n```\n\nThat's a lot of information! The numbers get very large very quickly - the 100th Fibonacci number is a 21-digit number. Let's see how different approaches to compression handle this much larger sequence.\n\n## The Building Blocks\n\nWe'll use two main classes to explore this idea:\n- `Rule`: Represents a production rule in our grammar, like \"S -\u003e A B\" meaning \"S can be replaced with A followed by B\"\n- `Grammar`: A collection of rules that can generate patterns, with methods to calculate how many bits we need to store it\n\nYou can find the implementation details in [`grammar.py`](/code/grammar.py).\n\n## Approach 1: Naive Grammar - Simple Memorization\n\nLet's start with the most straightforward approach - just writing down what we see:\n\n```python\n# Create a rule for each Fibonacci number\nnaive_rules = [Rule('S', ['F1'])] + [\n    Rule(f'F{i+1}', [str(num)])\n    for i, num in enumerate(fib_sequence)\n]\nnaive_grammar = Grammar(naive_rules)\nnaive_bits = naive_grammar.description_length()\nprint(f\"Naive grammar needs {naive_bits:,} bits\")\nprint(f\"Compression ratio: {direct_bits/naive_bits:.2f}x\")\n```\n```\nNaive grammar needs 892 bits\nCompression ratio: 2.97x\n```\n\nThis represents the most basic level of pattern recognition:\n- ‚úì Noticing that numbers can be labeled (F1, F2, etc.)\n- ‚úì Understanding sequence order\n- ‚úó No understanding of relationships between numbers\n- ‚úó No recognition of the underlying pattern\n\nIt's like a student who memorizes \"1, 1, 2, 3, 5, 8...\" without understanding why these numbers appear in this order. They achieve some compression just by being organized, but they're still essentially memorizing.\n\n## Approach 2: Pattern Recognition - Seeing Relationships\n\nNow we start to notice relationships between numbers. This requires more sophisticated observation:\n\n```python\npattern_rules = [\n    Rule('S', ['F', 'N1']),\n    Rule('N1', ['1']),\n] + [\n    Rule(f'N{fib}', [str(fib)])\n    for fib in sorted(set(fib_sequence[:20]))  # First 20 unique numbers\n] + [\n    Rule('F', ['N1']),\n    Rule('F', ['N1', '+', 'N1']),  # 1 + 1 = 2\n    Rule('F', ['N2', '+', 'N3']),  # 2 + 3 = 5\n    Rule('F', ['N3', '+', 'N5']),  # 3 + 5 = 8\n    Rule('F', ['N5', '+', 'N8']),  # 5 + 8 = 13\n]\npattern_grammar = Grammar(pattern_rules)\npattern_bits = pattern_grammar.description_length()\nprint(f\"Pattern grammar needs {pattern_bits:,} bits\")\nprint(f\"Compression ratio: {direct_bits/pattern_bits:.2f}x\")\n```\n```\nPattern grammar needs 428 bits\nCompression ratio: 6.19x\n```\n\nThis represents an intermediate level of understanding:\n- ‚úì Recognition that numbers are related through addition\n- ‚úì Ability to see specific instances of the pattern\n- ‚úó Still manually writing out each addition step\n- ‚úó No recognition of the recursive nature\n- ‚úó Can't generate numbers beyond what's explicitly coded\n\nIt's like a student who realizes \"Oh, I can get each number by adding specific previous numbers!\" They're starting to see relationships, but they're still writing out each step manually. They might even make a table:\n```\n1 + 1 = 2\n2 + 3 = 5\n3 + 5 = 8\n```\nThis is better than memorization, but they haven't yet had their \"aha!\" moment.\n\n## Approach 3: Full Abstraction - The Cognitive Leap\n\nFinally, we make the cognitive leap to understand the deep structure:\n\n```python\nabstract_rules = [\n    Rule('S', ['F', '1']),\n    Rule('F', ['1']),\n    Rule('F', ['F', '+', 'F_prev'])\n]\nabstract_grammar = Grammar(abstract_rules)\nabstract_bits = abstract_grammar.description_length()\nprint(f\"Abstract grammar needs {abstract_bits:,} bits\")\nprint(f\"Compression ratio: {direct_bits/abstract_bits:.2f}x\")\n```\n```\nAbstract grammar needs 14 bits\nCompression ratio: 189.21x\n```\n\nThis represents the highest level of understanding, requiring several cognitive breakthroughs:\n- ‚úì Recognition that each number depends on the previous TWO numbers\n- ‚úì Understanding that this single relationship explains EVERY number\n- ‚úì Grasping that you don't need to store the numbers themselves\n- ‚úì Realizing the pattern is recursive and self-contained\n- ‚úì Understanding that this works for ANY length sequence\n\nThe cognitive steps to reach this understanding typically involve:\n1. Noticing that you're always adding two numbers\n2. Realizing those two numbers are always the previous ones\n3. The key insight: this ONE rule explains EVERYTHING\n4. Understanding that with just this rule and a starting point, you can generate the entire sequence\n\nIt's like the student who suddenly exclaims \"Wait... we're ALWAYS just adding the last two numbers! That's all we need to know!\" This is the moment of true understanding, where the pattern becomes crystal clear and beautifully simple.\n\n## Generating the Sequence\n\nThe beauty of this abstract grammar is that it's not just for compression - we can use it to regenerate the original sequence or extend it to any length we want:\n\n```python\ndef generate_fibonacci(n: int) -\u003e List[int]:\n    \"\"\"Generate first n Fibonacci numbers using our grammar rules.\"\"\"\n    sequence = [1, 1]  # Initial state from our grammar's rules\n    for _ in range(n-2):\n        sequence.append(sequence[-1] + sequence[-2])  # F -\u003e F + F_prev rule\n    return sequence\n\n# Test the generator\nprint(\"Regenerating our sequence:\")\nprint(generate_fibonacci(10))  # First 10 numbers\nprint(\"\\nExtending beyond what we originally stored:\")\nprint(generate_fibonacci(15))  # First 15 numbers\n```\n```\nRegenerating our sequence:\n[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n\nExtending beyond what we originally stored:\n[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]\n```\n\nThis demonstrates the true power of understanding the generative process: with just 14 bits of grammar rules, we can:\n1. Reproduce the original sequence exactly\n2. Generate any Fibonacci number we want\n3. Extend the sequence infinitely\n\nThe massive improvement in compression ratio (189.21x vs 2.97x) reflects this deep understanding. We've gone from:\n- Memorizing 100 numbers (naive) ‚Üí\n- Understanding specific additions (pattern) ‚Üí\n- Grasping the universal rule (abstract)\n\nThis progression mirrors how humans learn: from memorization, to pattern recognition, to true understanding. The fact that better understanding leads to better compression isn't just a mathematical curiosity - it's a fundamental principle of how we make sense of the world.\n\nLet's visualize these compression ratios:\n\n```python\nprint(\"\\nCompression Comparison:\")\nprint(\"=\" * 50)\nprint(f\"Direct storage:     {direct_bits:6,} bits (baseline)\")\nprint(f\"Naive grammar:      {naive_bits:6,} bits ({direct_bits/naive_bits:6.2f}x better)\")\nprint(f\"Pattern grammar:    {pattern_bits:6,} bits ({direct_bits/pattern_bits:6.2f}x better)\")\nprint(f\"Abstract grammar:   {abstract_bits:6,} bits ({direct_bits/abstract_bits:6.2f}x better)\")\n```\n```\nCompression Comparison:\n==================================================\nDirect storage:      2,649 bits (baseline)\nNaive grammar:         892 bits (  2.97x better)\nPattern grammar:       428 bits (  6.19x better)\nAbstract grammar:       14 bits (189.21x better)\n```\n\nThis dramatic improvement shows the true power of finding the underlying generative process. The more data we have, the more valuable it becomes to understand the true pattern rather than just storing or partially compressing the output.\n\n## From Numbers to Language\n\nLet's apply the same thinking to language patterns. Here's a set of similar sentences:\n\n```python\nsentences = [\n    \"I like cats and dogs\",\n    \"I like books and music\",\n    \"I like coffee and tea\",\n    \"I like movies and games\"\n]\ndirect_bits = sum(len(s) * 8 for s in sentences)  # 8 bits per char\nprint(f\"Direct storage needs {direct_bits} bits\")\n```\n```\nDirect storage needs 688 bits\n```\n\nThat's a lot of bits! Let's try our different approaches:\n\n### Naive Grammar (Store Each Sentence):\n```python\nnaive_rules = [\n    Rule('S', ['I', 'like', 'cats', 'and', 'dogs']),\n    Rule('S', ['I', 'like', 'books', 'and', 'music']),\n    Rule('S', ['I', 'like', 'coffee', 'and', 'tea']),\n    Rule('S', ['I', 'like', 'movies', 'and', 'games'])\n]\nnaive_grammar = Grammar(naive_rules)\nnaive_bits = naive_grammar.description_length()\nprint(f\"Naive grammar needs {naive_bits} bits (compression ratio: {direct_bits/naive_bits:.2f}x)\")\n```\n```\nNaive grammar needs 63 bits (compression ratio: 10.92x)\n```\n\nJust by recognizing words as reusable symbols, we get almost 11x compression!\n\n### Pattern Recognition (Find Common Structure):\n```python\npattern_rules = [\n    Rule('S', ['I', 'like', 'THING', 'and', 'THING']),\n    Rule('THING', ['cats']), Rule('THING', ['dogs']),\n    Rule('THING', ['books']), Rule('THING', ['music']),\n    Rule('THING', ['coffee']), Rule('THING', ['tea']),\n    Rule('THING', ['movies']), Rule('THING', ['games'])\n]\npattern_grammar = Grammar(pattern_rules)\npattern_bits = pattern_grammar.description_length()\nprint(f\"Pattern grammar needs {pattern_bits} bits (compression ratio: {direct_bits/pattern_bits:.2f}x)\")\n```\n```\nPattern grammar needs 30 bits (compression ratio: 22.93x)\n```\n\nBy recognizing that we can reuse the pattern \"I like X and Y\", we get even better compression!\n\n### Full Abstraction (Separate First and Second Items):\n```python\nabstract_rules = [\n    Rule('S', ['I', 'like', 'N1', 'and', 'N2']),\n    Rule('N1', ['cats', 'books', 'coffee', 'movies']),\n    Rule('N2', ['dogs', 'music', 'tea', 'games'])\n]\nabstract_grammar = Grammar(abstract_rules)\nabstract_bits = abstract_grammar.description_length()\nprint(f\"Abstract grammar needs {abstract_bits} bits (compression ratio: {direct_bits/abstract_bits:.2f}x)\")\n```\n```\nAbstract grammar needs 39 bits (compression ratio: 17.64x)\n```\n\nInterestingly, trying to be too clever (separating first and second items) actually hurts our compression! This is a key insight: the best compression comes from matching the true structure of the data.\n\n### The Challenge of English Grammar\n\nHowever, English presents a challenge for such clean abstractions. Unlike our previous examples, English is a context-sensitive language, which means the interpretation of a word or phrase often depends on its context. For example:\n\n```python\ncontext_examples = [\n    \"The bank is closed\" ,           # Financial institution\n    \"The bank is muddy\",            # River bank\n    \"I bank on your support\",       # Rely/depend\n    \"The plane will bank left\"      # Aviation term\n]\n```\n\nThis context sensitivity means:\n1. The same word can have different meanings based on surrounding words\n2. The same grammatical structure can have different interpretations\n3. Valid combinations depend on semantic meaning, not just syntax\n\nThis limits how much we can compress English using pure grammatical rules. Our compression ratio of 22.93x was possible because we used a very restricted subset of English. In the general case, we would need:\n- Semantic context rules\n- Word sense disambiguation\n- Complex dependency structures\n\nThis is why formal languages (like programming languages) and some natural languages with more rigid structure can achieve better compression ratios.\n\n### Sanskrit: A More Compressible Language\n\nSanskrit, in contrast, was designed with formal grammar in mind. Its structure is more amenable to rule-based generation because:\n1. Words are derived from root forms using explicit rules\n2. Compound words follow clear compositional rules\n3. Sentence structure has more rigid constraints\n4. Word meanings are more systematically related to their roots\n\nThis makes Sanskrit more like our Fibonacci sequence - there are clear generative rules that can produce valid constructions.\n\n### Sanskrit Examples: Panini's Method in Action\n\nLet's look at some concrete examples of how Panini's grammar generates Sanskrit words and sentences:\n\n1. **Root-Based Word Generation**:\n   ```python\n   # Example: Generating forms of \"bh≈´\" (to be)\n   root_rules = [\n       Rule('VERB', ['ROOT', 'SUFFIX']),\n       Rule('ROOT', ['bh≈´']),  # \"to be\"\n       Rule('SUFFIX', ['ami']),  # 1st person present\n       Rule('SUFFIX', ['asi']),  # 2nd person present\n       Rule('SUFFIX', ['ati'])   # 3rd person present\n   ]\n   ```\n   This generates:\n   - bhavƒÅmi (I am)\n   - bhavasi (you are)\n   - bhavati (he/she/it is)\n\n   The transformation bh≈´ ‚Üí bhav is handled by another rule (gu·πáa strengthening), showing how Panini's grammar captures phonological changes systematically.\n\n2. **Compound Word Formation**:\n   ```python\n   # Example: Generating compound words\n   compound_rules = [\n       Rule('COMPOUND', ['WORD1', 'WORD2']),\n       Rule('WORD1', ['rƒÅja']),  # king\n       Rule('WORD2', ['putra'])  # son\n   ]\n   sandhi_rules = [\n       Rule('SANDHI', ['a', 'a'], ['ƒÅ']),  # a + a ‚Üí ƒÅ\n   ]\n   ```\n   This generates:\n   - rƒÅja + putra ‚Üí rƒÅjaputra (king's son)\n   The rules handle both combination and sound changes (sandhi).\n\n3. **Sentence Structure**:\n   ```python\n   # Example: Generating active/passive sentences\n   sentence_rules = [\n       Rule('S', ['NP', 'VP']),\n       Rule('NP', ['devadatta']),  # Devadatta (name)\n       Rule('VP', ['odana·πÉ', 'pacati']),  # rice + cooks\n       Rule('VP_PASSIVE', ['odana·∏•', 'pacyate'])  # rice + is cooked\n   ]\n   ```\n   This generates:\n   - devadatta·∏• odana·πÉ pacati (Devadatta cooks rice)\n   - odana·∏• pacyate (Rice is cooked)\n\nThe power of Panini's system comes from how these rules interact:\n\n1. **Recursive Application**:\n   ```python\n   # Example: Complex word formation\n   derivation_rules = [\n       Rule('WORD', ['ROOT', 'PRIMARY_SUFFIX', 'SECONDARY_SUFFIX']),\n       Rule('ROOT', ['bh≈´']),\n       Rule('PRIMARY_SUFFIX', ['ana']),  # action noun\n       Rule('SECONDARY_SUFFIX', ['tva'])  # abstract quality\n   ]\n   ```\n   This generates:\n   - bh≈´ ‚Üí bhavana (becoming) ‚Üí bhavanatva (the quality of becoming)\n\n2. **Meta-Rules**:\n   ```python\n   # Example: Rule ordering\n   meta_rules = [\n       Rule('APPLY_ORDER', ['ROOT_RULES', 'SANDHI_RULES', 'ACCENT_RULES']),\n       Rule('EXCEPTION', ['if_final_position', 'skip_sandhi'])\n   ]\n   ```\n   These meta-rules ensure correct application order and handle exceptions systematically.\n\nThis systematic approach achieves remarkable compression because:\n1. Each rule can generate many forms (e.g., one verb root rule generates hundreds of conjugations)\n2. Rules interact to produce complex forms (like compounds and derivatives)\n3. Meta-rules handle exceptions without needing separate rules for each case\n4. The system captures both form (phonology) and meaning (semantics)\n\nFor example, from just the root \"bh≈´\" and a set of rules, Panini's grammar can generate:\n- All conjugated forms (bhavƒÅmi, bhavasi, bhavati, etc.)\n- All derived nouns (bhavana, bhƒÅva, bh≈´ti, etc.)\n- All compounds (bh≈´loka - world of existence, bh≈´tap≈´rva - having been before, etc.)\n- All these forms in different syntactic roles\n\nThis is analogous to our Fibonacci example, where one simple rule (Fn = Fn-1 + Fn-2) generates an infinite sequence. But Panini's grammar goes further, handling multiple interacting patterns simultaneously while maintaining semantic coherence.\n\n## From Numbers to Panini: The First Computational Grammarian\n\nAfter seeing how we derived grammars for the Fibonacci sequence and explored language patterns, let's look at a remarkable historical parallel: Panini's Ashtadhyayi, written around 500 BCE. This ancient work presents Sanskrit grammar as a generative system, much like our examples above.\n\n### How Panini Might Have Done It\n\nLooking at our own process of grammar derivation:\n1. Start with examples (like our Fibonacci numbers or sentences)\n2. Look for patterns (like our pattern recognition phase)\n3. Abstract to rules (like our final grammars)\n\nPanini likely followed a similar path:\n1. **Data Collection**: Gathered thousands of Sanskrit utterances\n2. **Pattern Recognition**: Identified recurring structures\n3. **Rule Abstraction**: Derived minimal generative rules\n4. **Optimization**: Compressed rules for memorization\n\nHis grammar achieves remarkable compression:\n- ~4,000 rules can generate all of Sanskrit\n- Rules are so concise they fit in ~40 pages\n- Can generate millions of valid word forms\n- Includes phonological, morphological, and syntactic levels\n\n### The First \"Compression as Intelligence\"\n\nWhat makes Panini's work particularly relevant to our discussion is that it demonstrates the same principles we've discovered:\n\n1. **Cognitive Progression**:\n   - Like our Fibonacci example progressing from naive to abstract\n   - From memorizing words to understanding derivation rules\n\n2. **Minimal Description**:\n   - Like our abstract Fibonacci grammar using just 3 rules\n   - His grammar captures an entire language in ~4,000 rules\n\n3. **Generative Power**:\n   - Like our abstract grammars generating infinite sequences\n   - His system can generate all valid Sanskrit constructions\n\n### From Ancient Grammar to Modern Systems\n\nWhile Panini's work on Sanskrit grammar might seem purely academic, his approach to finding minimal generative rules has profound implications for modern complex systems. Consider:\n\n1. **Pattern Recognition**: Just as Panini identified patterns in language, we can identify patterns in any system\n2. **Rule Abstraction**: His method of finding minimal rules can apply to any complex behavior\n3. **Generative Power**: Like his grammar generating infinite valid sentences, we can generate infinite valid states\n\nThis approach becomes particularly powerful when we apply it to modern systems that seem vastly different from language. Let's see how Panini's principles can help us understand and compress something seemingly unrelated: a video game.\n\n## Extending to State Machines: The Case of Pong\n\nThe real power of Panini's compression approach to generative grammars becomes clear when we apply it to modelling more complex systems with real-world state machines - like video games. Just as Panini found that all of Sanskrit could be generated from ~4,000 rules, we'll see how an entire game can be generated from just 8 rules. Let's look at how we could compress a recording of a Pong game:\n\n```python\nclass PongGrammar:\n    def __init__(self):\n        self.rules = [\n            Rule('GAME', ['INIT', 'LOOP']),\n            Rule('INIT', ['Ball(center)', 'Paddle1(mid)', 'Paddle2(mid)', 'Score(0,0)']),\n            Rule('LOOP', ['UPDATE', 'COLLISIONS', 'SCORE', 'LOOP']),\n            Rule('UPDATE', ['Ball(pos += vel)', 'Paddle1(pos += input1)', 'Paddle2(pos += input2)']),\n            Rule('COLLISIONS', ['WALL_CHECK', 'PADDLE_CHECK']),\n            Rule('WALL_CHECK', ['if ball.y \u003e height: ball.vel.y *= -1']),\n            Rule('PADDLE_CHECK', ['if ball.collides(paddle): ball.vel.x *= -1']),\n            Rule('SCORE', ['if ball.x \u003c 0: p2_score++ else if ball.x \u003e width: p1_score++']),\n        ]\n        self.grammar = Grammar(self.rules)\n\n# Let's calculate sizes for a 5-minute game at 60 FPS\nframes = 300 * 60  # 5 minutes at 60 FPS\n\n# First, let's look at a conservative estimate (just storing positions)\npos_frame_size = (4 + 4 + 8)  # 4 bytes each for paddles, 8 for ball position\npos_raw_size = pos_frame_size * frames\ngrammar_size = pong.grammar.description_length()  # Rules of the game\ninput_size = 2 * frames  # 1 bit per paddle per frame\nstate_size = 16  # 4 bytes each for ball x,y and two paddle positions\n\nprint(\"Conservative Estimate (Position Data Only)\")\nprint(\"=========================================\")\nprint(f\"Raw position data:   {pos_raw_size:,} bytes ({pos_raw_size/1024:.1f} KB)\")\nprint(f\"Grammar + state:     {grammar_size + input_size + state_size:,} bytes ({(grammar_size + input_size + state_size)/1024:.1f} KB)\")\nprint(f\"Compression ratio:   {pos_raw_size/(grammar_size + input_size + state_size):.2f}x\")\n\n# Now let's look at the full visual data\nframe_height = 210\nframe_width = 160\nchannels = 3  # RGB\npixel_frame_size = frame_height * frame_width * channels  # bytes per frame\npixel_raw_size = pixel_frame_size * frames  # Full video storage\n\nprint(\"\\nFull Visual Data Estimate (Every Pixel)\")\nprint(\"=======================================\")\nprint(f\"Frame dimensions: {frame_width}x{frame_height} pixels (RGB)\")\nprint(f\"Storage requirements:\")\nprint(f\"- Raw video:        {pixel_raw_size:,} bytes ({pixel_raw_size/1024/1024:.1f} MB)\")\nprint(f\"- Grammar rules:    {grammar_size:,} bytes\")\nprint(f\"- Player inputs:    {input_size:,} bytes\")\nprint(f\"- Game state:       {state_size:,} bytes\")\nprint(f\"- Total compressed: {grammar_size + input_size + state_size:,} bytes ({(grammar_size + input_size + state_size)/1024:.1f} KB)\")\nprint(f\"Compression ratio:  {pixel_raw_size/(grammar_size + input_size + state_size):,.2f}x\")\n\n# To put this in perspective:\nprint(\"\\nTo store a 5-minute Pong game:\")\nprint(\"1. Conservative Approach (Just Positions)\")\nprint(f\"   - Raw data: {pos_raw_size/1024:.1f} KB\")\nprint(f\"   - Compressed: {(grammar_size + input_size + state_size)/1024:.1f} KB\")\nprint(f\"   - Ratio: {pos_raw_size/(grammar_size + input_size + state_size):.2f}x better\")\nprint(\"\\n2. Full Visual Approach (Every Pixel)\")\nprint(f\"   - Raw data: {pixel_raw_size/1024/1024:.1f} MB\")\nprint(f\"   - Compressed: {(grammar_size + input_size + state_size)/1024:.1f} KB\")\nprint(f\"   - Ratio: {pixel_raw_size/(grammar_size + input_size + state_size):,.2f}x better\")\n```\n```\nConservative Estimate (Position Data Only)\n=========================================\nRaw position data:   1,872,000 bytes (1,828.1 KB)\nGrammar + state:     38,064 bytes (37.2 KB)\nCompression ratio:   49.18x\n\nFull Visual Data Estimate (Every Pixel)\n=======================================\nFrame dimensions: 160x210 pixels (RGB)\nStorage requirements:\n- Raw video:        18,144,000,000 bytes (17,304.7 MB)\n- Grammar rules:    2,048 bytes\n- Player inputs:    36,000 bytes\n- Game state:       16 bytes\n- Total compressed: 38,064 bytes (37.2 KB)\nCompression ratio:  476,671.45x\n\nTo store a 5-minute Pong game:\n1. Conservative Approach (Just Positions)\n   - Raw data: 1,828.1 KB\n   - Compressed: 37.2 KB\n   - Ratio: 49.18x better\n\n2. Full Visual Approach (Every Pixel)\")\n   - Raw data: 17,304.7 MB\n   - Compressed: 37.2 KB\n   - Ratio: 476,671.45x better\n```\n\nThis is remarkable in two ways:\n\n1. **Conservative Estimate** (Just storing positions):\n   - Raw data: ~1.8 MB\n   - Compressed: ~37 KB\n   - Nearly 50x compression just for the game state!\n\n2. **Full Visual Data** (Every pixel of every frame):\n   - Raw data: ~17.3 GB\n   - Compressed: ~37 KB\n   - Almost 500,000x compression!\n\nEven our conservative estimate shows impressive compression because we're capturing the underlying game logic. But when we consider that this same grammar can generate the complete visual output, the compression becomes staggering. We achieve this because:\n\n1. The rules of the game (our grammar) - about 2KB\n2. The player inputs - about 35KB\n3. The game state - just 16 bytes\n\nThe grammar captures both the physics engine and the rendering logic in just 8 rules! This massive compression ratio illustrates a profound point: when we truly understand a system, we don't need to store its behavior - we can store its rules and regenerate any behavior we want. This is exactly how human intelligence works: we don't memorize every position or pixel of every object we've ever seen moving - we learn the laws of physics and can use them to predict and understand any motion.\n\n## The Connection to Intelligence\n\nThe relationship between compression and intelligence becomes clear when we look at our progression from simple sequences to complex systems. In each case, better compression came from better understanding of the underlying pattern:\n\n1. For Fibonacci, understanding the recursive relationship led to 189.21x compression\n2. For language, recognizing sentence structure led to 22.93x compression (limited by English's context sensitivity)\n3. For Sanskrit, Panini's grammar achieved remarkable compression of an entire language with just ~4,000 rules\n4. For Pong, applying Panini's principles to game physics led to 476,671.45x compression\n\nThis progression - from numbers to language to video games - shows how the same fundamental principles of finding minimal generative descriptions apply across domains. This is why compression can be seen as a measure of intelligence: the better we understand a system, the more efficiently we can describe it.\n\nIn machine learning terms, this is closely related to the concept of \"minimum description length\" (MDL) principle, which states that the best model for a dataset is the one that minimizes:\n1. The size of the model (our grammar rules)\n2. The size of the data when encoded using the model (our inputs)\n\nThis principle was later formalized by Ray Solomonoff in his theory of universal inductive inference. Solomonoff showed that the best prediction for future data comes from the shortest computer program that can generate the observed data. In other words, finding the most compressed representation of data (like Panini's grammar rules) isn't just an efficiency trick - it's mathematically optimal for prediction and understanding.\n\nThe connection to Solomonoff induction helps explain why Panini's grammar has remained useful for over two millennia: by finding the shortest possible rule set that could generate Sanskrit, he wasn't just being clever with compression - he was discovering the true underlying structure of the language. This is exactly what Solomonoff's theory predicts: the shortest description that works is likely to be the correct one.\n\nThis is exactly what we've demonstrated with our code examples above and what Panini achieved with Sanskrit. His work isn't just limited to modelling the spoken linguistic reality of the time - but was about discovering a universal principle of intelligence that holds the key to building thinking machines of the future.\n\n## Why This Matters\n\nThis connection between compression and understanding has profound implications:\n\n1. **Learning** is essentially finding better grammars for observed data\n2. **Intelligence** can be measured by ability to find compact descriptions in the fewest observations (e.g. by looking at the least number of Fibonacci numbers)\n3. **Understanding** means finding the true generative process\n\nWhen a child learns physics, they're not memorizing the position of every object they've seen - they're learning the grammar of motion. When we understand language, we don't memorize every possible sentence - we learn the rules that generate valid ones. When Panini created his grammar, he wasn't just documenting Sanskrit - he was discovering a fundamental approach to understanding that we can apply to everything from ancient languages to modern video games.\n\nThis is why generative grammars are so powerful: they don't just compress data, they capture the underlying processes that created that data. Whether it's a sequence of numbers, a set of sentences, or a video game, the principle remains the same: understanding the rules of generation is the key to both compression and comprehension.\n\n\u003e \"Riemann invented his geometries before Einstein had a use for them; the physics of our universe is not that complicated in an absolute sense.  A Bayesian superintelligence, hooked up to a webcam, would invent General Relativity as a hypothesis‚Äîperhaps not the dominant hypothesis, compared to Newtonian mechanics, but still a hypothesis under direct consideration‚Äîby the time it had seen the third frame of a falling apple.  It might guess it from the first frame, if it saw the statics of a bent blade of grass.\" - E. Yudkowsky\n\n## References\n\n[1] [Youtube video](https://www.youtube.com/watch?v=dO4TPJkeaaU), Compression for AGI, Jack Rae, Stanford MLSys, ex-OpenAI, 2023\n\n[2] [Youtube Playlist](https://www.youtube.com/playlist?list=PLsAPTmdVuspykLNnjs1_zQKRMqRRfDr2R), PƒÅ·πáini Lecture Series, Dr. Saroja Bhate, Bangalore International Center, 2023\n\n[3] The *A·π£·π≠ƒÅdhyƒÅyƒ´* achieves a remarkable compression ratio of at least 5000:1, condensing the rules that can generate over 10,000 hours of attested Sanskrit literature (including the ~20,000 verses of the four Vedas, along with the 100,000 verses of the Mahabharata, 24,000 verses of the Ramayana, 400,000 verses of the Puranas, and hundreds of thousands of verses across texts, philosophical shastras, and classical poetry) into just 2 hours of precisely formulated rules.\n\n[4] PƒÅ·πáini: Catching the Ocean in a Cow's Hoofprint, Vikram Chandra, 2019[blog.granthika.co/panini/](https://blog.granthika.co/panini/) ","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/panini_old":{"title":"The PƒÅ·πáinian Approach to World Modelling","content":"\u003e \"The simplest of several competing explanations is likely to be the correct one\" - Occam's Razor\n\n## Intelligence as compression\n\nLet‚Äôs examine a modified version of Searle‚Äôs Chinese room experiment. Suppose we have 5.6TB of text data, ethically scraped from the internet, broken into word pairs (2-grams) stored in a lookup table. When asked to complete \"I was going to wear a...\", it might meaninglessly output \"a lot\" because \"a lot\" appears more frequently than \"a shirt\" or \"a skirt\". A 3-gram model, using two words of context, improves accuracy but still fails in cases like \"It‚Äôs raining outside, wear a...\". Both are examples of generative models: they predict the statistically-most-likely next word based on patterns in the data.\n\nLarge Language Models (LLMs) like DeepSeek, GPT, or Claude are more sophisticated versions of such generative models. They use thousands of tokens of context and the 'attention' mechanism to focus on relevant parts of the input to 'query' with.  They're not just storing a memorized table of what comes after what, but using compute to extract and store a hiearchy of reusable chunks of information in it's layers. A result is that the weights of the Llama-65B model occupy around 365GB on disk, down from the 5.6TB it's trained on (a 14x compression)[1]. We can see that generalization ability and data efficiency are equivalent: generalization comes from squeezing every bit of information out of your datapoints, understanding all correlations and causations, and connecting all the dots. ‚ÄúSqueezing every bit of information‚Äù is meant literally: generalization is the very direct result of compression. \n\nYet, LLMs are still many orders-of-magnitude less data-efficient than humans. Lee Sedol, a top Go player, played around 10,000 games in his lifetime, while DeepMind's bot AlphaGo required 30 million games to match him. If Sedol had played 30 million games, how skilled would he be? What would a human who has absorbed all of human knowledge look like? How does the human generative model compress information so effectively? The answer to these question is the key to building machines that think, learn, adapt to tasks like (or better than) humans do, i.e. general machine intelligence.\n\nBut it's hard to run this experiment because most humans have seen orders of magnitude less data in their lifetimes, and it's hard to manually inspect human priors. Or so I thought, until I attended the lecture series[2] by Dr. Saroja Bhate at Bangalore International Centre, on PƒÅ·πáini (pronounced \"pah-nee-nee\"), the ancient Sanskrit grammarian. Over 2300 years ago, before the advent of computers or formal logic, PƒÅ·πáini sat down and methodically reduced all of human knowledge, then floating around in spoken Vedic Sanskrit, into a generative grammar of exactly 3,995 *s≈´tras*, or rewrite rules - recorded in his magnum opus, the *A·π£·π≠ƒÅdhyƒÅyƒ´*. These rules have remained unchanged ever since.\n\nThere is a small pool of postdoctoral Sanskrit scholars who are fortunate to understand the full magnitude of PƒÅ·πáini's achievement, but I will try to do some justice to it. Faced with a large corpus of spoken Vedic and contemporary Sanskrit, many thousands of hours of audio signals collected without any substrate to record with or automated tooling to work with, PƒÅ·πáini, over 12 years, found abstracted atoms of meaning that when combined with a set of dynamic rules and meta-rules formed a generative grammar, a deterministic state machine that could be used to re-synthesize the original audio corpus - and be recited in just 2 hours (an astonishing compression ratio of atleast ~5000:1)[3].  \n\nHow is PƒÅ·πáini's ~4000 rule set a compressed generative model like one would understand an LLM to be? Imagine prompting an LLM like ChatGPT with an empty prompt, and letting it run for a few paragraphs. If you did this a trillion times, with slightly different temperature settings, you would eventually recover a slightly morphed version of the entire corpus it was trained on. 'Model distillation' is the industry term for this common practice, used to copy parts of another LLM's training set. In a similar manner, if you were to run the state machine described in the *A·π£·π≠ƒÅdhyƒÅyƒ´* a billion times with random inputs, you would eventually recover all the Vedic hymns, mantras, and Brahmanas it compresses, include the different variations of spoken Sanskrit. Of course, just like you guide an LLM's output by including an input prompt, you would need to guide the state machine in the *A·π£·π≠ƒÅdhyƒÅyƒ´* to generate the *kind* of sentences you wanted. But the complexity of the original source remains contained in the abstracted, highly compressed rule set.\n\n\u003e \"If the universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset\" - Ray Solomonoff, inventor of algorithmic probability\n\nPƒÅ·πáini gives the formation of every inflected, compounded, or derived word, with an exact statement of the sound-variations (including accent) and of the meaning. His foresight in designing these rules means they have stood the test of time - not only expressive enough to explain the knowledge of the past at the time, but also to generalize to phrases and sentences in the future since then. His work is the first formal system known to man, doing to linguistic reality what Euclid would go on to do later for geometry, but it would be no overstatement to call it the most impressive human act of knowledge compression till date. It is the only example we have of true optimal [Solomonoff induction](https://en.m.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference), finding the shortest executable archive of a dataset, as evidenced by it's durability over time.\n\nOf course, PƒÅ·πáini was working with Sanskrit, whose underlying structure makes it less context-sensitive than English and more amenable to such decomposition. But for the sake of comparison, his methods if automated and applied to the [Hutter compression prize](http://prize.hutter1.net/) dataset could compress 1GB of Wikipedia data to a few kilobytes (down from the current record of 110MB as of Feb 2025). A digital superintelligence in action, would very much employ what could be called \"PƒÅ·πáini's Razor\" - applying the dual techniques of abstraction and economy ruthlessly to thousands of noisy signals of various forms and fidelity, reducing them by many orders of magnitude into succinct set of formal predicates - deriving an explanation unchanging in time. Like breaking down a house into basic Lego-like blocks and then building a new house from it back gain, a machine like this could then combine these discovered reusable concepts on the fly using abstracted transformation rules to generalize to any unknown. \n\n\u003e \"The descriptive grammar of Sanskrit which PƒÅ·πáini brought to it's highest perfection is one of the greatest monuments of human intelligence\" - L. Bloomfield, father of American distributionalism\n\nIt's worth examining the implications of this. To build powerful thinking machines of the future that can compress information and generalize better than humans do, we must dig up and study in great detail the forgotten relic that is PƒÅ·πáini's Razor. \n\n## PƒÅ·πáini's Razor\n\nDisclaimer : I have no formal training in Sanskrit, and the following is merely a programmer's attempt to reverse-engineer PƒÅ·πáini's methods of compression. All mistakes are mine, all brilliance his.\n\nThis is largely a guide for programmers, so we dive into code right away. Let's first look at how generative grammars compress information.\n\n\n\n[to be continued...] Source[4]\n\n[1] [Youtube video](https://www.youtube.com/watch?v=dO4TPJkeaaU), Compression for AGI, Jack Rae, Stanford MLSys, ex-OpenAI, 2023\n\n[2] [Youtube Playlist](https://www.youtube.com/playlist?list=PLsAPTmdVuspykLNnjs1_zQKRMqRRfDr2R), PƒÅ·πáini Lecture Series, Dr. Saroja Bhate, Bangalore International Center, 2023\n\n[3] The *A·π£·π≠ƒÅdhyƒÅyƒ´* achieves a remarkable compression ratio of at least 5000:1, condensing the rules that can generate over 10,000 hours of attested Sanskrit literature (including the ~20,000 verses of the four Vedas, along with the 100,000 verses of the Mahabharata, 24,000 verses of the Ramayana, 400,000 verses of the Puranas, and hundreds of thousands of verses across texts, philosophical shastras, and classical poetry) into just 2 hours of precisely formulated rules.\n\n[4] PƒÅ·πáini: Catching the Ocean in a Cow‚Äôs Hoofprint, Vikram Chandra, 2019[blog.granthika.co/panini/](https://blog.granthika.co/panini/)\n\n","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/pirate":{"title":"The Parable of The Robot Pirate","content":"You're AI researcher Barbadosa the pirate trainer, and the first day at the job you have to teach a new robot pirate (a deep learning model, say, JackSparrowGPT), how to find treasure at sea. If you make one of these and make infinite copies of it, you will become very rich, very quick.  \n  \nOf course, you don't have a boat yet and have never actually ventured out to sea yourself, but you do have the entire island to yourselves. So much training data to train it using gradient descent. You also have infinite compute.\n  \nSo this robot pirate, armed with a matter scanner, goes around the island inspecting every rock, every leaf, every tree.  \n  \nSometimes it comes across a puddle and scans it. He puts his feet, records what wetness feels like, and steps out. \"Oh this is a great example of a water body!\", You exclaim. Surely this is meaningful progress at building treasure-finding systems. \"I'm going to be very rich\" you exclaim.  \n  \nIt sometimes disturbs you how much resource JackSparrowGPT requires, unlike your own brain which runs on 20W of power and can learn from just a few samples. The robot pirate improves only linearly with exponential increase in data and compute you throw at it - but luckily, you have so much of it! Soon it has inspected and memorized single molecule and atom on the island. But these systems are huge and unwieldy to update, so you have to cutoff it's data at a certain date, it can't take in unlimited memory after all.  \n  \nYou create new benchmarks to test these systems. For instance, the Tidal Wave benchmark, given the water level on Tuesday, it can predict the chances of high tide on Wednesday. And because the robot has years of data about tides, of course it can make a big guess, and he gets it right once, twice, thrice. You are impressed. The smartest guy you know can also make accurate tidal wave predictions. Maybe the robot is as smart as him.  \n  \nAfter years of training it on the dynamics of every single atom on the island, you discover it has completely beaten all the island-based benchmarks. It can beat the most talented island habitants at tree climbing, puddle hopping and stone throwing. You decide that it is now time.  \n  \nYou point the robot towards the horizon and say : \"You see that line in the distance. there are great treasures beyond that line. you have to go find them and bring them back\"  \n  \nThe robot promptly agrees. It starts walking towards the sea. For the first few steps, its just like walking on island, and the robot just beat the best human walker on the \"Walking\" benchmark last year. It continues to walk, until a wave comes in lifts it off its feet. There was never any water body or experience on the island similar to this, so the robot has no idea what to do. It errors out, spluttering and choking, as it drowns, and you watch on in horror.  \n\nBut surely you can augment it with information storage and retrieval, use it's vast understanding of things on the island to embed new data not seen before - and then search through it to make a new plan for the sea. Or - the robot understands \"wetness\" and \"water body depth\", because it has a embedded vector for \"puddles\" somewhere in it's latent space, based on it's understanding of puddles the island. You can use that to understand \"the sea\" as well.\n\nYou let it loose on the sea again - but alas, while the robot can vectorize 'water body depth' of the sea, there was a property \"saline concentration\" that it never encountered in the puddles on the island. As it goes deeper into the sea, the salt gets everywhere and the circuits malfunction. You watch in horror again as it drowns.\n\nAs it turns out, the sea (like real life) is an unbounded stream of unknown, unseen evaluation data, that a robot pirate bounded by scaled-up linear regression just cannot adapt to. It has to generalize in a provably unbounded / universal manner [[1]](/notes/unbounded)\n\nYou go back to the drawing board. Backpropagation is not the way. Gradient descent must be ditched. You `rm -rf` CUDA and start from scratch.\n\n---\n  \n_This is the underwhelming state of modern machine learning, and surprisingly, what the hype / doomer AI cults are excited / worried about. There is little reason to get excited by / fear bounded robot pirates._","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/questions":{"title":"questions","content":"\nThis is a list of questions I'm tortured by. Most of the answers might be related and/or build on top of another. Ping me if you're thinking about these too.\n\n## intelligence \n\n1. what does it mean for a machine to feel something, or, have a soul?\n2. what will the the first superintelligence in the first few minutes of existence look like? \n3. what will it care about? \n4. what will be the kind of predictions / generalization power the first superintelligence will have?\n4. how will intelligent machines and humans co-exist before, and during a post-scarcity society?\n5. how could one align the interests of a superintelligence with those of it's creators?\n\n## cognitive Science \n\n1. how does the human brain manage to do so much with so less?\n2. why does the octopus brain look so much like the mammalian brain in structure and function even though they took different evolutionary paths?\n3. why and how did we evolve to dream?\n4. how could one way approximate a mind in-silico?\n5. how does the human mind mine abstractions to find relations between unrelated things?\n6. could a person blind from birth solve the [ARC-AGI](https://arcprize.org/) challenge / [Bongard problems](https://www.oebp.org/welcome.php)?\n\n## language\n\n1. why is it that only humans are able to form accumulating state over generations via language?\n2. why can't feral children who spend their early years in the wild acquire language when they return to civilization?\n3. why and how could Panini discover a rigorous formal generative grammar for a organic, spoken, chaotic oral language?\n4. what happens in the brain when you read a joke, a story, or a sentence that evokes a strong a emotion?\n5. could thinking machines communicate, extend and form accumulating state in ways better than lossy context-sensitive grammars?\n6. what are the whales talking to each other about?\n7. Sapir‚ÄìWhorf hypothesis : mechanistically, why/how could language effect the perception of reality/time? \n8. Are there custom languages that could be designed for thinking about particular problems (like CNC coding logic)?\n\n## space exploration \n\n1. what does the tech tree to industrialize/colonize the galaxy look like?\n2. what does the first self-replicating probe to colonize space look like?\n3. what will a step-by-step roadmap to construct the first Dyson swarm around the sun include?\n4. why does it seem like we are alone in the universe?\n\n## reality\n\n1. why and how did the universe start and how will it end, if at all?\n2. what unifies the discrete quantum scale with the continuous macroscopic scales?\n3. how can two quantum particles on two ends of the universe be understood as if they were right on top of each another i.e. non-locality?\n4. what is the true nature of causality in a non-local universe?\n5. how could local perception of space and time emerge from a non-local underlying reality?\n6. what happens when a quantum superposition collapses upon measurement? \n7. why would reality be a simulation?\n8. how would one build a universe like ours on a computer?\n\n## metaphysics / miscellaneous\n\n1. why is there something rather than nothing?\n2. why are we here?\n3. how could we eliminate the last source of scarcity - cycle of life and death?\n4. what straight line can we draw to explain the human condition, and how does it extrapolate into the future?\n5. how did the first self-replicating living organism come into being? \n6. how would one create life from scratch in sandbox, say Minecraft?\n7. is there any possible character decision tree which would allow CJ from GTA San Andreas to break out of the game?\n8. if you were living at a certain level of a Mandelbrot set, how would you figure out the function which generated the reality around you?\n\n\n \n\n\n","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]},"/notes/unbounded":{"title":"Unbounded Generality","content":"A thought is a program, and so viewed from the lens of information theory, there are broadly three distinct universalities in entities that can think and generalize [[1]](https://twitter.com/dela3499/status/1656437115901624322) : \n\n1. Computational - can run any given program\n2. Reachability - can eventually generate any program, taking arbitrary steps in the process\n3. Comparison - can compare and select program options using any criteria\n\nWhile arguably these universalities are at play for all sorts of tasks - from the mundane to the profound - their utility becomes truly clear when one examines the problem of [extreme L3 generalization](https://blog.mayalabs.io/benchmark), or discovering new knowledge that does not resemble anything in past training experience.\n\nNew discovery requires finding an underlying explanation for two or more things that seem to have nothing in common, and then using that explanation to extrapolate a prediction about how the world works. For instance, Newton's law of gravitation states that when the distance from an attracting mass is doubled, the force decreases to one fourth. His flash of genius was in realizing that celestial and earthly gravitation had the same origin.\n\nThis sort of discovery needs a causal understanding of [unrelated things](https://twitter.com/sibeshkar/status/1657329757158281216), a world model of them that can be malleably updated on the go, with sparse data, in order to compare and find a common graph of meaning between them. \n\nSay a digital agent was tasked with discovering a theory of everything - the elusive explanation that unifies the discrete small scales and continous large scales of the reality we perceive. It would need to synthesize hypothesis programs and test them against observations in both quantum mechanics and general relatively. This is a difficult, time-taking process, for a couple of reasons : \n\n1. The agent would need to poke the world along axes that are not indexed in any training data / are not represented in any geometric space\n2. The symbols/rules for expressing such an explanation likely don't exist yet, and definitely not in sufficient quantity to learn via rote\n3. The tools for building a simulated world model at the right level of abstraction would need to be constructed by the agent - not very different from Einstein's thought experiment of travelling on a light beam\n4. The synthesis process will require compressing the evolution of both QM and GR into one recursive program that models the evolution of arbitrary stretches of time, in order to find an isomorphism\n\nIf/when an isomorphism is found among pairs of such programs, that would unify both these fields. Such a common program subgraph will then have the predictive power to make new hypotheses and discoveries we don't know about yet.\n\nToday's methods are nowhere near being capable at such universality. Starting with the right questions is the first step to building them one day.","lastmodified":"2025-03-04T06:29:26.522901463Z","tags":[]}}